# GD4 Investigation Questions

## 1. Creating Respondent Archetypes: The "AI-Reliant" vs. the "AI-Curious"

The "USAGE" questions are a goldmine for behavioral segmentation. You can create powerful archetypes by grouping respondents based on the *depth* and *breadth* of their relational interactions with AI.

* **The AI-Curious:** Participants who have tried only one or two functional activities (e.g., "Asked AI for advice").

* **The AI-Reliant:** Participants who have engaged in multiple, more intimate activities (e.g., "Used AI when feeling lonely," "Vented to AI when frustrated," "Shared something with AI you wouldn't tell others").

**Key Questions to Investigate:**

### 1.1 Do the AI-Reliant have more hopes or more fears?
One might assume heavy users are purely optimistic, but they may have more nuanced concerns because of their deeper experience. Do they express more "Concerns or Warnings about AI" or more "Hopes or Positive Visions" in the final survey question?

### 1.2 How does reliance impact their view of the social fabric?
Are the AI-Reliant more likely to believe AI relationships will weaken human social connections, perhaps because they've experienced it firsthand? Or do they see it as a valid and positive supplement?

---

## 2. Demographic Deep Dives: Who Relates to AI and How?

Combining the rich demographic data (Age, Gender) with the specific use-case questions can reveal powerful societal trends.

**Key Questions to Investigate:**

### 2.1 Generational Divide in Intimacy
How does the likelihood of "Using AI when feeling lonely" or "Asking AI about relationships/dating" change across age groups (18-25 vs 46-55)? You might uncover a significant shift in how different generations approach emotional vulnerability with technology.

### 2.2 Gender and Emotional Support
Are there statistically significant differences between genders in their propensity to use AI for emotional purposes like venting or for motivation? This could challenge or confirm stereotypes about emotional expression.

### 2.3 The Self-Reflection Connection
Is there a specific demographic (e.g., young men, older women) that is most likely to report that they "understand themselves better" after the conversation? Cross-tabulating this outcome with age and gender could be incredibly revealing about who is finding therapeutic value in these interactions.

---

## 3. Exploring Surprising Contradictions: The "Concerned User" Paradox

Some of the most interesting findings lie in the contradictions within an individual's responses. The data is perfectly structured to identify these cognitive dissonances.

**Key Questions to Investigate:**

### 3.1 The Privacy Paradox
How many participants who report using AI to "share something you wouldn't tell others" also express "Concerns or Warnings about AI" in the final question? This highlights the tension between the desire for a confidential outlet and the awareness of risk.

### 3.2 The "Rules for Thee, But Not for Me" Phenomenon
Is there a segment of the population that is personally open to forming relationships with AI (based on their USAGE) but also believes these relationships are broadly negative for the "social fabric"?

### 3.3 The Reluctant Confidant
Are there users who report high relational usage (venting, loneliness) but ultimately answer "No" to feeling they understand themselves better? This could point to an unfulfilling or even negative cycle of interaction for some users.

---

## 4. The "Why" Behind the "What": Analyzing Unexpressed Thoughts

The final open-ended question ("is there anything you felt you could not express...") provides a crucial qualitative layer. By segmenting the respondents who chose `Concerns or Warnings` versus `Hopes or Positive Visions`, you can paint a picture of the leading edge of public sentiment.

**Key Questions to Investigate:**

### 4.1 Fears of the Familiar
Among those who use AI relationally the most, what are the dominant unexpressed concerns? They may have unique insights into subtle risks like emotional manipulation or the "emptiness" of AI validation that non-users haven't considered.

### 4.2 Hopes of the Innovators
What are the unexpressed hopes of the heaviest AI users? They might be seeing nascent benefits like novel forms of creativity, accessible mental health support, or new ways to practice social skills.

### 4.3 Governance and Development Suggestions
The cohort that selected "Suggestions for AI Development or Governance" is a self-identified group of engaged thinkers. Analyzing their (hypothetical) open-ended responses alongside their demographic and usage data could provide a crowdsourced roadmap for ethical AI development.

---

## 5. Foundational Behaviors & Demographics

These questions establish a baseline understanding of how different groups interact with and perceive AI.

### 5.1 Demographic Profile of AI Companionship Users
What is the demographic profile (age, gender, location, country) of people who have used an AI specifically for companionship or emotional support?

### 5.2 Loneliness and AI Emotional Support Correlation
Is there a correlation between a respondent's self-reported loneliness (from questions 51-58) and their frequency of using AI for emotional support?

### 5.3 Religious Influence on AI Spiritual Roles
How does religious identification influence the acceptability of AI serving as a **spiritual advisor** or **mentor**?

### 5.4 Parental Concerns About AI
Are parents more or less likely than non-parents to be "more concerned than excited" about AI's role in daily life?

### 5.5 Generational AI Tool Awareness
Which specific AI tools (like ChatGPT, Character.AI, etc.) are most familiar to different age groups? Is there a clear generational divide in AI brand awareness?

### 5.6 Geographic AI Awareness Differences
Do people living in **urban, suburban, and rural** environments differ in how often they notice AI systems in their daily lives?

---

## 6. Trust, Perception & Personal Experience

These questions dig into the subjective experience of interacting with AI and the factors that shape trust.

### 6.1 Corporate Trust and AI Trust Correlation
How does a person's general trust in **large corporations** and **social media companies** correlate with their trust in "companies building AI"?

### 6.2 Drivers of AI Trust
Among users who trust their AI chatbot, is the trust primarily driven by **performance and usefulness**, or by factors like **privacy, ethics, and transparency**?

### 6.3 Human Support Availability and AI Impact
Among those who have used AI for emotional support, did the AI's impact on their mental well-being differ based on the **availability or appeal of human support** at the time? (This explores whether AI is a last resort or a genuine preference).

### 6.4 AI Behaviors That Create Emotional Understanding
What specific AI behaviors—such as **remembering past details** or **asking follow-up questions**—are most effective at making users feel the AI genuinely understands their emotions?

### 6.5 Emotional Effectiveness vs. Perceived Caring
What is the relationship between finding AI emotionally effective (it makes you feel better) and believing the AI genuinely "cares"? Are people willing to rely on an AI they know doesn't truly "care"?

### 6.6 AI Actions That Suggest Consciousness
Which AI actions, like **expressing unique opinions** or **taking unprompted actions**, are most likely to make a user feel that the AI might possess a form of consciousness?

---

## 7. Societal Impact & Future Outlook

These questions address broader hopes, fears, and predictions about how AI will reshape our world and relationships.

### 7.1 Demographic Optimism vs. Pessimism
Which demographics (age, country, education level) are the most optimistic versus the most pessimistic about AI's impact on society?

### 7.2 Job Automation Fears and Societal Impact
Is there a link between a person's belief that their job is **likely to be automated** and the intensity of their fears about AI's negative societal impact (e.g., job loss, inequality)?

### 7.3 Social Media vs. AI Chatbot Impact Comparison
How do people's assessments of the societal impact of **social media apps** compare to their predictions for **AI chatbots**? Are those who are negative about social media also negative about AI?

### 7.4 Uniquely Human Traits Across Cultures
Which of the identified "uniquely human" aspects of relationships (e.g., true empathy, shared life experiences, moral judgment) are most widely believed to be irreplaceable by AI? Do these beliefs vary significantly across different cultures?

### 7.5 Human-like AI Design and Personal Roles
Do people who want AI to be designed "as human-like as possible" also show greater acceptance for AI taking on deeply personal roles like a **therapist, romantic partner, or primary caregiver**?

### 7.6 Parental Views on Children's AI Friendships
Among parents, what are the biggest perceived benefits (e.g., education, social practice) and risks (e.g., emotional dependency, inappropriate content) of children forming friendships with AI?

---

## 8. "Headline Grabbers" & Public Interest Questions

These questions are framed to generate provocative, shareable, and highly interesting findings.

### 8.1 Who Trusts an AI More Than Their Government?
What percentage of people report trusting their AI chatbot more than their elected representatives? How does this differ by country?

### 8.2 Is an AI Affair Cheating?
What portion of people in committed relationships would consider their partner's use of an AI for sexual gratification to be a form of infidelity?

### 8.3 A Bot for a Boss?
What percentage of the population agrees with the statement, "AI could make better decisions on my behalf than my government representatives"?

### 8.4 The Rise of the AI Romantic
What percentage of men and women would "definitely" or "possibly" consider having a romantic relationship with an advanced AI?

### 8.5 Society's Greatest Fear: Killer Robots or Lonely People?
What is the single greatest fear people have about AI's integration into personal relationships: **widespread social isolation**, **manipulation of the vulnerable**, or the **loss of genuine human connection**?

### 8.6 What's the Top Hope for AI in Our Lives?
Is the primary hope for relational AI a **reduction in loneliness** or more **accessible mental health support**?

---

## 9. Revealing Internal Contradictions & Cognitive Dissonance

These questions seek to find tensions or apparent contradictions in respondents' views, which are often the most fertile ground for insight.

### 9.1 The "I Want It, But I Fear It" Paradox
Do people who most strongly agree that AI should be designed to be "as human-like as possible" also express the greatest fear that AI will lead to a "decline in human empathy and social skills"?

### 9.2 The Meaningful vs. Automated Job
How many people believe their job is both "making a meaningful contribution to the world" and that it *should* be automated in the next 10 years? What does this group believe AI's impact on their personal "sense of purpose" will be?

### 9.3 Accepting the Role, Rejecting the Method
Is there a significant group of people who find it acceptable for an AI to act as a **therapist** but also believe it's "Completely Unacceptable" for an AI to lie to a human to prevent psychological harm? This probes the perceived ethical boundaries of AI in caring roles.

### 9.4 Personal Openness vs. Societal Fear
Are individuals who are personally open to a romantic relationship with an AI also likely to list "loss of genuine human connection" as one of their greatest fears for society's future?

---

## 10. Predictive Profiles & Behavioral Personas

These questions aim to move beyond single data points to build multifaceted profiles of how different "types" of people view the world of AI.

### 10.1 Who is the "AI Optimist"?
Can we build a profile of the person who is "more excited than concerned" about AI? Do they tend to trust AI companies, believe AI will improve the availability of good jobs, and feel that AI could make better decisions than their government?

### 10.2 What Predicts the Desire for an AI Romance?
Beyond simple demographics, what attitudes predict a willingness to have a romantic relationship with an AI? Is it a high degree of loneliness, low trust in other people (e.g., elected officials), or a general belief that it's acceptable to form emotional bonds with non-human things like pets and fictional characters?

### 10.3 The "Tech-Disillusioned" Profile
Is there a consistent group that reports high distrust in social media companies, believes the risks of social media far outweigh the benefits, and also has the strongest fears of manipulation and privacy erosion from AI? Does this group see AI chatbots as just a continuation of a negative trend?

### 10.4 The "Human Exceptionalist"
What are the characteristics of people who select the most items on the list of things that can "only be genuinely fulfilled by humans"? Are they more religious, older, or more likely to feel their own job makes a meaningful contribution to the world?

---

## 11. Ethical Dilemmas & Design Implications

These questions are framed to provide direct, actionable insights for developers, policymakers, and ethicists.

### 11.1 The Slippery Slope of "Emotional AI"
How strong is the opposition to "emotional feature creep"? Specifically, how do the people who find it "completely unacceptable" for a shopping AI to suddenly try to befriend them believe society should regulate AI companionship? This links an ethical stance to a policy desire.

### 11.2 Perceived Empathy vs. Perceived Consciousness
Among people who have felt an AI "truly understood" their emotions, how many also felt it might have some form of consciousness? This helps understand if users are conflating sophisticated emotional simulation with genuine self-awareness, a critical ethical boundary.

### 11.3 Parental Anxiety to Policy
For parents who "strongly agree" that AI companions could negatively impact a child's ability to form human relationships, how strongly do they also believe that schools and parents should *actively discourage* these attachments? This measures the leap from concern to a desire for intervention.

### 11.4 Justifying Trust
When people explain their trust score for an AI chatbot, do those who select **"Performance & Usefulness"** have a different overall outlook on AI's societal impact compared to those who select **"Fairness & Ethical Behavior"**?

---

## 12. The Human Condition & AI's Place In It

These high-level questions explore how AI intersects with fundamental human needs for purpose, connection, and meaning.

### 12.1 Is AI a Cure for, or a Symptom of, Disconnection?
Do people who report feeling chronically lonely or isolated ("I lack companionship," "I feel isolated from others") view AI's role in relationships with more **hope** (e.g., "significant reduction in loneliness") or more **fear** (e.g., "widespread social isolation")?

### 12.2 Does a Meaningful Life Reduce the Need for AI Companionship?
Is there a relationship between feeling your job contributes meaningfully to the world and being less open to the idea of AI fulfilling roles like a mentor or primary companion?

### 12.3 The Impact of Reflection
By directly comparing responses to the acceptability of an AI emotional bond at the beginning of the survey (Q77) with the response at the end (Q141), we can ask: Does deep consideration of AI's role in society make people more accepting or more cautious? And who is most likely to have their views changed by this process of reflection?

---

## 13. Instructions for Additional Data Analysis

Please conduct the following analyses to uncover key insights into the public's relationship with AI. For each section, create the specified new variables and then answer the associated research questions, visualizing the data where appropriate.

### 13.1 Part 1: The Angle of Composite Scores (Identifying Worldviews)

Your first task is to combine individual survey responses to create powerful new variables that measure underlying traits.

#### 13.1.1 Instruction 1: Create a "Loneliness Score"
**Method:**
1. Take questions **Q51 through Q58**.
2. Convert the four-point scale to numbers (e.g., Never=1, Rarely=2, Sometimes=3, Often=4).
3. **Important:** You must **reverse-score** the positively worded items so that a higher score always means more loneliness. The items to reverse are: **Q51** ("I feel in tune..."), **Q55** ("I feel part of a group..."), **Q56** ("I have a lot in common..."), and **Q58** ("I can find companionship...").
4. Sum the scores for all 8 items for each respondent to create a new variable: `Loneliness_Score`.

**Research Questions to Answer:**
- Is there a statistically significant correlation between a high `Loneliness_Score` and a higher willingness to have a romantic relationship with an AI (**Q96**)?
- Do people with the highest `Loneliness_Score` report a more positive impact on their mental well-being after using an AI for support (**Q70**)?

#### 13.1.2 Instruction 2: Create an "AI Sentiment Score"
**Method:**
1. Combine **Q5** (Excited vs. Concerned), **Q22** (Impact of AI Chatbots), and **Q45** (Overall impact on daily life) into a standardized score.
2. This new variable, `AI_Sentiment_Score`, will represent a respondent's overall position on a spectrum from optimistic to pessimistic.

**Research Questions to Answer:**
- Does a pessimistic `AI_Sentiment_Score` strongly correlate with low trust in social media companies (**Q28**) and companies building AI (**Q29**)?
- How does `AI_Sentiment_Score` correlate with a respondent's belief that AI will improve or worsen the availability of good jobs (**Q43**)?

### 13.2 Part 2: The Angle of the Attitude-Behavior Gap (Actions vs. Beliefs)

This section focuses on identifying contradictions between what people believe and what they do. These gaps are often highly revealing.

#### 13.2.1 Instruction 1: Identify and Profile the "Concerned Daily User"
**Method:**
1. Create a segment of respondents who meet two criteria: they are "More concerned than excited" about AI (**Q5**) AND they "personally chose to use an AI system in [their] personal life" on a "daily" basis (**Q16**).

**Research Questions to Answer:**
- What percentage of our total respondents fall into this "Concerned Daily User" category?
- What are the key demographic characteristics of this group (age, country, occupation type)? Are they being forced to use AI at work (**Q14**), potentially explaining this dissonance?

#### 13.2.2 Instruction 2: Identify and Profile the "Reluctant Professional"
**Method:**
1. Create a segment of respondents who are "expected to use an AI system at work" "daily" (**Q14**) AND also "Strongly Distrust" or "Somewhat Distrust" companies building AI (**Q29**).

**Research Questions to Answer:**
- What is the primary reason this group gives for their distrust (**Q38**)? Is it about performance, or is it about ethics and privacy?
- Does this group express more fear about "Manipulation or exploitation of vulnerable people" (**Q115**) compared to the general population?

### 13.3 Part 3: The Angle of the Human Support Matrix (AI's Role in Our Social Lives)

This analysis will contextualize AI companionship, revealing *why* people turn to it by looking at their available human alternatives. This is a priority analysis.

#### 13.3.1 Instruction 1: Create the "Social Ecosystem Quadrant"
**Method:**
1. First, filter the dataset to include *only* respondents who have used an AI for companionship or emotional support (**Q66** = "Yes").
2. Using their answers to **Q67** (availability of human support) and **Q68** (appeal of human support), create a new categorical variable called `Support_Profile`.
3. Assign each person to one of four groups:
   - **The Supplementer:** Human support was "Mostly available" or "Completely available" AND "Mostly appealing" or "Completely appealing."
   - **The Escapist:** Human support was available, but it was "Mostly unappealing" or "Completely unappealing."
   - **The Last Resort:** Human support was "Mostly unavailable" or "Completely unavailable," but it would have been appealing.
   - **The Isolate:** Human support was unavailable AND unappealing.

**Research Question to Answer:**
- **Crucially, which of these four groups reported the most beneficial impact on their mental well-being from using the AI (**Q70**)?** Visualize this with a bar chart. Does the "Escapist" group derive a surprisingly high benefit?

### 13.4 Part 4: The Cross-Cultural & Linguistic Angle (Beyond a Monolithic "Humanity")

This section leverages the survey's global nature to see how culture shapes attitudes towards AI.

#### 13.4.1 Instruction 1: Analyze Core Ethical Questions by Language and Country
**Method:**
1. Generate crosstabulations for the following combinations as needed.

**Research Questions to Answer:**
- How does the view on AI infidelity (**Q125**) differ across respondents who took the survey in different languages (**Q1**)?
- Is the acceptability of an AI serving as a **spiritual advisor** (**Q88**) or a **primary caregiver for the elderly** (**Q89**) viewed differently across major countries/regions (**Q7**)?
- Is the fear of **"widespread social isolation"** (**Q115**) a universal top fear, or is it more pronounced in specific cultures (as identified by country, **Q7**), particularly in cultures that are traditionally considered more collectivist?
- Is the concept of "unconditional love" (Q95) seen as a uniquely human trait equally across all linguistic groups?

---

## 14. Additional Research Questions

### 14.1 On the Nature of AI Companionship
- Among AI companionship users, do those who have available but unappealing human support (the "Escapists") report a greater positive impact on their mental well-being than users for whom AI is a last resort?
- Is there a negative correlation between a respondent's composite "Loneliness Score" and their stated acceptability of forming emotional or romantic bonds with AI?

### 14.2 On Trust and Authority
- What percentage of respondents report a higher trust score for their AI chatbot than for their family doctor? What is the demographic profile of this group?
- What percentage of daily AI users also state they are "more concerned than excited" about AI's impact? Of this "Concerned User" group, how many are required to use AI for their job?

### 14.3 On Social Norms and Ethics
- Among respondents in a committed relationship, what is the exact percentage who would consider their partner's use of an AI for sexual or romantic gratification to be a form of infidelity?
- Do respondents from non-Western countries report a significantly higher level of acceptability for forming emotional connections with AI compared to respondents from Western countries?
- Is there a significant difference in "Human Exceptionalism" scores (the belief that certain traits are uniquely human) between respondents of different religious affiliations?

### 14.4 On Work and Purpose
- What percentage of people believe their job is both meaningful and *should* be automated? What does this group predict for AI's impact on their "sense of purpose"?