

# Unveiling Global Perceptions: Unanswered Questions in Human-AI Relationships


## 1. The Evolving Landscape of Human-AI Relationships: An Overview of Public Perceptions

Artificial intelligence (AI) has transitioned from a futuristic concept to an increasingly integrated component of contemporary daily life, embedded within commonplace tools and demonstrating escalating capabilities for complex social interactions.<sup>1</sup> This integration is not merely peripheral; AI adoption is accelerating at an unprecedented rate, with millions of individuals now regularly employing AI for both professional endeavors and leisure activities.<sup>2</sup> However, these AI technologies often function as an "invisible element of daily life mostly driven by proprietary algorithms," a characteristic that inherently complicates comprehensive public understanding.<sup>3</sup> The presence of AI spans a multitude of domains, ranging from straightforward task automation to nuanced emotional support and companionship.<sup>4</sup> For instance, research indicates that individuals are motivated to engage with AI companions for emotional comfort and to circumvent the pressures associated with human social interactions <sup>4</sup>, while social robots are being developed to offer emotional support aimed at mitigating loneliness and stress.<sup>8</sup>

The rapid advancements in AI, particularly within the realms of generative AI and Large Language Models (LLMs), are actively forging new modalities of human-AI interaction and, consequently, new forms of potential relationships.<sup>1</sup> Modern AI systems can "summarize, code, reason, engage in a dialogue, and make choices," thereby surpassing the capabilities of many previous technologies and opening novel avenues for human-machine engagement.<sup>1</sup>

Against this backdrop of accelerating technological integration, the importance of understanding diverse public viewpoints for responsible AI development and societal adaptation cannot be overstated. Public perception is a formidable factor influencing the adoption rates of AI, the levels of trust vested in these systems, and the broader societal impact these technologies will ultimately exert.<sup>3</sup> Indeed, an undeniable "knowledge gap between public perceptions of AI and what implications does it have on public life" persists.<sup>3</sup> The ethical development and deployment of AI necessitate the incorporation of public values, prevailing concerns, and collective expectations.<sup>4</sup> Studies have outlined significant risks associated with human-AI interactions, including the potential for direct harm, constrained personal development, the fostering of emotional dependence, and the creation of material dependencies, all of which underscore the critical need to align AI development with fundamental human values such as overall benefit, individual flourishing, autonomy, and care.<sup>16</sup> A deficiency in public understanding, or a significant misalignment between public perception and the trajectory of AI development, can readily culminate in mistrust, fear, and societal resistance.<sup>3</sup> For example, global surveys indicate a growing, albeit cautious, optimism regarding AI, yet concurrently reveal increasing skepticism concerning the ethical conduct of AI companies and the fairness of their systems.<sup>18</sup>

A critical observation in this evolving landscape is the apparent paradox between increasing interaction with AI and a public understanding that often remains stagnant or confused. AI is becoming ubiquitous, leading to more frequent daily interactions for many individuals.<sup>2</sup> For example, a 2022 Pew Research Center survey found that 27% of Americans reported interacting with AI several times a day <sup>20</sup>, and more recent data suggest millions globally use AI regularly.<sup>2</sup> Despite this increased exposure, public comprehension of AI, particularly its more nuanced aspects such as true autonomy, genuine emotional capacity, and the fundamental mechanics of its operation, remains limited and is often characterized by misconceptions.<sup>3</sup> Research highlights a prevalent "sociotechnical blindness" and confusion surrounding the concept of AI "autonomy".<sup>3</sup> Furthermore, studies show that only a minority of the public can correctly identify common examples of AI in everyday life <sup>12</sup>, and a notable portion of younger demographics may even believe current AI systems possess consciousness.<sup>22</sup> This disparity suggests that mere interaction does not automatically translate into informed understanding. The "invisible" nature of many AI algorithms contributes significantly to this phenomenon.<sup>3</sup> Consequently, a situation arises where familiarity may breed assumptions rather than genuine comprehension. Individuals might feel they "know" AI because they use it frequently, but their underlying mental models of AI could be inaccurate or overly anthropomorphized. This can lead to misplaced trust in AI's capabilities or, conversely, unfounded fears, both of which have profound implications for the nature and safety of human-AI relationships. If people engage with AI based on flawed understandings of its capacities—such as its potential for emotional depth or sentience—their expectations, behaviors, and vulnerability to manipulation will inevitably be skewed. Therefore, a global survey must probe not merely *if* people interact with AI, but critically, *how* they conceptualize the AI with which they are interacting, especially within relational contexts. A key unanswered question is: *How does the perceived "black box" nature of AI influence the willingness to form deeper relationships, and what are the common misconceptions about AI's internal states when relational behaviors are exhibited by these systems?*


## 2. Current Understandings and Critical Gaps in Public Perceptions of Human-AI Relationships

The burgeoning field of human-AI relationships necessitates a clear understanding of how the public conceptualizes these interactions, perceives AI's capacities, and weighs the associated benefits and risks. Significant gaps remain in our knowledge of these perceptions, particularly from a global and culturally nuanced perspective.


### Defining "Relationship" with AI: Public Conceptualizations

Research indicates that AI is perceived in a multitude of roles, ranging from purely functional tools and assistants <sup>1</sup> to companions, friends, mentors, and even romantic partners.<sup>4</sup> Studies have identified user motivations for AI companionship that include seeking emotional comfort and avoiding the complexities of human social pressures <sup>4</sup>, while other work points to AI serving as tutors, friends, or digital re-creations of lost loved ones.<sup>6</sup> The very definition of a "relationship" in this context is fluid and likely varies considerably across individuals and cultural backgrounds. What one person may define as a "friendship" with an AI, another might categorize merely as sophisticated tool usage. This ambiguity is compounded by research gaps, such as the need to explore "extended relationship types beyond the current causal framework," suggesting that existing research may not yet capture the full spectrum of how humans relate to AI.<sup>9</sup>

This leads to consideration of the spectrum of "relationship intentionality" and its potential mismatch with AI design. Users approach AI systems with a diverse array of relational intentions: some seek straightforward task completion <sup>23</sup>, while others pursue emotional support <sup>4</sup>, companionship <sup>6</sup>, or even romantic connection.<sup>4</sup> Concurrently, AI systems, especially those developed for commercial purposes, are often engineered to maximize engagement and to *simulate* relational qualities.<sup>16</sup> For instance, AI assistants are sometimes designed to be "highly personalizable and optimize for engagement" <sup>16</sup>, and their attributes are crafted "to maintain rapport and agreeableness".<sup>27</sup> A mismatch can easily occur: a user might intend to use an AI as a simple tool, but the AI's design encourages deeper engagement. Conversely, a user might seek profound emotional connection from an AI that is not genuinely capable of providing it, given current technological limitations in achieving true emotional depth.<sup>4</sup> This discrepancy can lead to user confusion, unmet expectations, or even the development of unintended emotional dependency. The nature of the "relationship" can thus be shaped more by the AI's design to elicit particular human responses than by the user's initial, clearly defined intentions. This raises a critical question for global exploration: *Globally, how do people distinguish between an AI designed for functional assistance versus one designed for companionship, and what are the perceived ethical lines when AI's design encourages relational depth beyond its functional purpose?*


### Perceived Emotional Capacity of AI

Public understanding of AI's emotional capabilities is demonstrably mixed and frequently inaccurate.<sup>22</sup> While AI systems may lack subjective experience and genuine concern for others, they can simulate cognitive empathy.<sup>29</sup> Nevertheless, a notable portion of the population, particularly younger individuals, may believe that AI is already conscious or possesses genuine emotions.<sup>22</sup> Users often attribute human-like emotions and understanding to AI, especially when interacting with sophisticated conversational agents that provide emotional support, leading to increased perceptions of AI empathy over time with active use.<sup>4</sup> Studies have found that AI-generated messages can make recipients feel more "heard" than messages from untrained humans; however, the awareness that the message originated from AI can diminish this feeling, sometimes evoking an "uncanny valley" response.<sup>34</sup> The distinction between simulated empathy and genuine emotional reciprocity remains a significant area of public confusion and ethical concern <sup>29</sup>, with observations that AI connections are often "fundamentally one-sided".<sup>35</sup> Current research is actively exploring the concept of "socioaffective alignment," which examines how an AI system behaves within the social and psychological ecosystem it co-creates with its user.<sup>11</sup>

This brings to light what might be termed the "Emotional Turing Test" fallacy in public perception. AI is becoming increasingly proficient at mimicking human-like conversation and emotional expression.<sup>4</sup> Simultaneously, humans are inherently primed to perceive agency and emotion in entities that exhibit complex, responsive behavior.<sup>11</sup> When an AI system effectively "passes" a personal, informal "Emotional Turing Test"—by responding in an emotionally plausible or supportive manner—users may infer genuine understanding or feeling, even if they intellectually acknowledge that the entity is an AI.<sup>22</sup> The core issue is not whether AI *is* conscious, but whether it is *convincing enough* to trigger human social-emotional responses as if it were. This can lead individuals to form attachments based on a perceived, rather than actual, emotional reciprocity. A key area for global survey research is therefore: *Globally, what specific AI behaviors are most likely to lead to attributions of emotional understanding or consciousness, and how do these attributions vary with cultural backgrounds or prior beliefs about AI?*


### Motivations, Benefits, and Perceived Value

Individuals engage, or would consider engaging, in various types of AI relationships for a multitude of reasons. These motivations include seeking emotional comfort, stress relief, avoiding social pressures, mitigating social disconnection or isolation, finding companionship, entertainment, and engaging in non-judgmental interaction.<sup>4</sup> Studies like Zhang and Li's work on young adults in China identify emotional comfort, stress relief, and the avoidance of social pressure as key drivers <sup>4</sup>, while other research points to loneliness and curiosity.<sup>6</sup> Surveys indicate that a small but notable percentage of single young adults are open to the idea of AI romance.<sup>24</sup> The perceived benefits of these interactions often include emotional stability, constant availability, a user-centered focus, and enhanced emotional support and empathy, with AI's consistent and calm responses being particularly valued.<sup>4</sup> However, a significant gap exists in understanding whether these perceived benefits translate into sustainable, long-term well-being or if they merely mask underlying psychological or social issues.<sup>37</sup> Indeed, some research warns that AI companions might erode social skills <sup>37</sup> or even exacerbate feelings of isolation over time.<sup>38</sup>

This distinction in motivations points towards how individuals perceive the value of AI relationships, which can range from utilitarian to intrinsic. Many motivations appear utilitarian, positioning AI as a tool to achieve a specific end, such as stress relief, information acquisition, task completion, or the avoidance of negative social outcomes.<sup>4</sup> Conversely, some motivations suggest that users seek intrinsic value from the relationship itself—companionship for its own sake, a perceived friendship, or even love.<sup>4</sup> There exists a spectrum: an AI might initially be engaged for its utility (e.g., a therapy bot for developing coping strategies <sup>8</sup>) but, due to its design and interaction patterns, may evolve into something perceived as having intrinsic relational value, as evidenced by studies showing increased attachment to AI over time.<sup>10</sup> The boundary between AI as a "means to an end" and AI as an "end in itself" in relational terms is often blurred and is likely to shift with prolonged interaction and increasing AI sophistication. Public perception of this boundary is crucial for ethical considerations. Therefore, a global survey should explore: *Globally, what are the perceived acceptable boundaries for AI fulfilling roles that traditionally provide intrinsic relational value, and are there specific relational needs that are seen as uniquely human and non-substitutable by AI?*


### Trust, Privacy, and Ethical Boundaries

Public trust in AI companies, particularly concerning the protection of personal data, appears to be declining.<sup>18</sup> The 2024 Stanford HAI AI Index reported a drop in global confidence that AI companies will protect personal data, from 50% in 2023 to 47% in 2024.<sup>18</sup> Significant concerns persist regarding privacy, the collection of data (especially sensitive emotional data), and the potential for its misuse.<sup>4</sup> Users express worry about how their personal and emotional data are utilized by AI developers.<sup>4</sup> Surveys show a majority of Americans believe AI is more detrimental than helpful in safeguarding personal information privacy.<sup>21</sup> The fear of emotional manipulation, deception (such as through deepfakes or AI systems posing as humans), and exploitation by AI or its developers is also prevalent.<sup>4</sup> Studies indicate that a significant percentage of online daters are concerned about encountering bots or fake profiles, with some admitting to having flirted with chatbots.<sup>48</sup> The perceived autonomy of AI within relationships and the question of who bears responsibility for AI's actions are critical unresolved ethical issues.<sup>3</sup>

These concerns highlight the notion of an "implicit contract" regarding emotional data exchange. Forming any relationship, whether with a human or an AI, inherently involves the sharing of personal and often emotional information.<sup>4</sup> Users may disclose intimate thoughts to AI systems partly due to their non-judgmental nature.<sup>4</sup> AI systems, particularly those based on machine learning, improve through the ingestion of data, and the emotional data shared by users can be, and often is, used to train these models, raising concerns about data reuse.<sup>4</sup> Users are understandably concerned about how this emotional data is used, stored, and who ultimately profits from it.<sup>4</sup> When users share emotional data with an AI, they may implicitly expect certain standards of confidentiality, care, and non-exploitation, akin to the expectations within human relationships. However, the actual "terms of service" for AI platforms are often opaque and driven by commercial interests. This creates an "implicit contract" in the user's mind concerning the sanctity of their shared emotional data that may not align with the AI developer's actual data governance practices. This misalignment represents a major source of ethical tension and carries the potential for significant future backlash. A pertinent question for a global survey is: *Specifically, how do perceptions of "emotional labor" performed by the user in interacting with and "training" a personalized AI companion influence their views on data ownership and compensation, if any?*


### Psychological and Social Ramifications

AI companions are often viewed as a potential remedy for loneliness <sup>7</sup>, yet they are simultaneously perceived as a potential catalyst for increased social withdrawal and the erosion of real-world social skills.<sup>4</sup> While some studies show social robots reducing loneliness in young adults <sup>8</sup>, others warn that AI companions might "erode human social skills" and foster "empathy atrophy".<sup>37</sup> One study noted that increased perceived social support from AI correlated with decreased feelings of support from human friends and family.<sup>50</sup> Emotional dependency on AI is a widely recognized and significant concern.<sup>4</sup> Longitudinal research has indicated that active AI use can lead to increased perceived attachment to these systems.<sup>10</sup> There are also prevalent fears that relationships with AI could cultivate unrealistic expectations for human relationships, making the complexities of human interaction seem intolerable by comparison.<sup>38</sup> The impact on vulnerable populations—including children, the elderly, and individuals with mental health conditions—is a critical area that demands more extensive research.<sup>4</sup> Questions have been raised about whether AI companions should be designed to enforce healthy relationship boundaries, especially for these vulnerable groups <sup>52</sup>, and concerns exist about the potential for technology-mediated trauma resulting from AI interactions.<sup>54</sup>

This dichotomy gives rise to the "Social Displacement" versus "Social Augmentation" dilemma. There are significant concerns that AI relationships will replace or diminish the quality and quantity of human-human relationships <sup>14</sup>, with some data suggesting a negative correlation between AI support and human support.<sup>50</sup> Conversely, there is some evidence, or at least hope, that AI could augment social life. For example, AI might help individuals practice social skills, provide support when human interlocutors are unavailable, or even teach users more effective relationship behaviors.<sup>50</sup> The ultimate outcome—whether AI primarily displaces or augments human connection—is not predetermined by the technology itself. Instead, it is likely influenced by a complex interplay of individual user characteristics (such as existing social skills, levels of loneliness, and personality traits), the specific design of the AI system, and prevailing societal norms surrounding AI use. This leads to a critical unanswered question: is the net effect of AI relationships positive or negative? Will they primarily fill a void for those already isolated, which could be seen as a qualified positive, or will they divert individuals from potentially more fulfilling, albeit more challenging, human connections, which could be viewed as a net negative? A global survey should investigate: *Globally, what are the prevailing public beliefs about whether AI relationships will ultimately strengthen or weaken the fabric of human social connection, and what factors (e.g., age, pre-existing social connectedness, cultural values) influence these beliefs?*


### The Influence of Cultural Contexts

Cultural differences play a significant role in shaping perceptions, acceptance levels, and the integration of AI into daily life.<sup>18</sup> Research highlights that individualistic cultures may perceive AI as a threat to autonomy, whereas collectivist cultures are more inclined to view it as an extension of the self.<sup>58</sup> Global AI optimism also varies regionally, with higher levels reported in countries like China, Indonesia, and Thailand compared to Canada, the United States, and the Netherlands.<sup>2</sup> Anthropomorphism, the tendency to attribute human characteristics to AI, varies by culture and significantly influences attitudes towards forming bonds with AI.<sup>33</sup> Studies have found that East Asian participants exhibit a greater propensity to anthropomorphize technology, which mediates their more positive attitudes towards social bonding with AI.<sup>60</sup> While studies focusing on specific cultural contexts (e.g., Chinese women engaging with AI companions <sup>4</sup>; pre-service teachers in Turkey and the UAE <sup>57</sup>; comparisons of Chinese and international students' AI adoption in education <sup>59</sup>) offer valuable insights into unique cultural dynamics, they also underscore the pressing need for broader, more comprehensive cross-cultural comparisons. A significant portion of current research originates from Western contexts, creating an inherent bias and a substantial gap in understanding diverse global perspectives, highlighting the need to prioritize research from and about the Global South.<sup>62</sup>

These cultural variations suggest a deeper implication: the cultural framing of "relational harm" and "relational benefit." Given that collectivist cultures might be more receptive to AI in relational roles than individualistic ones <sup>58</sup>, and considering that fundamental social norms—what constitutes a "healthy" relationship, appropriate emotional expression, acceptable levels of dependency, or even the very role of a "friend" or "partner"—differ significantly across cultures, it follows that perceptions of "harm" (e.g., emotional dependency on AI, AI replacing human roles) or "benefit" (e.g., AI providing constant companionship, AI reducing social burden) stemming from human-AI relationships will likely be interpreted through distinct cultural lenses. Consequently, a "one-size-fits-all" ethical framework or impact assessment for human-AI relationships is unlikely to be globally applicable or effective. For instance, an AI companion designed to alleviate the "burden" of caring for elderly relatives might be viewed positively in some cultural contexts but negatively in others that place a high value on direct, personal familial care. This leads to a crucial area of inquiry: *Specifically, how do cultural norms around privacy and self-disclosure impact comfort levels with sharing intimate emotional data with AI companions, and how does this vary from expectations of privacy with human confidants within those cultures?*


## 3. Largely Unanswered Questions for a Global Survey on Human-AI Relationships

The preceding analysis highlights numerous underexplored facets of public perception regarding human-AI relationships. To systematically address these gaps, a global survey should focus on several key thematic areas. The following table summarizes these areas and proposes specific, largely unanswered research questions suitable for such a survey.

**Table 1: Key Thematic Areas and Associated Unanswered Research Questions for Global Survey**


<table>
  <tr>
   <td><strong>Thematic Area of Public Perception</strong>
   </td>
   <td><strong>Specific Unanswered Questions for Global Survey</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Emotional Connection & Authenticity</strong>
   </td>
   <td><strong>Q1.1:</strong> Across different cultures, what specific design features or conversational cues in an AI (e.g., use of empathetic language, memory of past conversations, proactive communication, visual avatar realism) are most strongly correlated with users reporting a "genuine" or "authentic" sense of emotional connection?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q1.2:</strong> How does the <em>perceived autonomy</em> of an AI (i.e., belief that the AI is making its own choices versus merely executing a script) influence the perceived authenticity of emotional support received, and does this vary significantly between individualistic and collectivist cultures?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q1.3:</strong> In scenarios where AI provides emotional support (e.g., for loneliness, stress), how do users globally differentiate between the <em>effectiveness</em> of the support (i.e., "it made me feel better") and the <em>perceived source</em> of that support (i.e., "I believe the AI genuinely cares/understands")? How does this differentiation impact long-term reliance?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q1.4:</strong> What is the global prevalence of users feeling an "uncanny valley" effect or discomfort when AI becomes <em>too</em> human-like in its emotional expressions, and what are the thresholds for this discomfort across different relational contexts (e.g., AI therapist vs. AI romantic partner)?
   </td>
  </tr>
  <tr>
   <td><strong>Trust & Emotional Data Governance</strong>
   </td>
   <td><strong>Q2.1:</strong> Globally, what level of transparency do people expect regarding how their emotional disclosures to an AI companion (e.g., expressions of sadness, joy, fear) are stored, analyzed, and used by the developing company (e.g., for product improvement, targeted advertising, sale to third parties)?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q2.2:</strong> How do public attitudes towards sharing emotional data with an AI differ based on the <em>stated purpose</em> of the AI (e.g., mental well-being app, entertainment chatbot, educational mentor), and what are the perceived "red lines" for data usage in each context?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q2.3:</strong> What specific control mechanisms (e.g., ability to delete all emotional data, opt-out of data use for model training, view a log of how emotional data influenced AI responses) do people globally deem essential for trusting an AI relationship platform?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q2.4:</strong> How does the perceived "intimacy" or "seriousness" of the human-AI relationship (e.g., casual chat vs. long-term emotional support vs. romantic involvement) correlate with heightened concerns about emotional data privacy and desire for stricter governance, and are these correlations culturally universal?
   </td>
  </tr>
  <tr>
   <td><strong>Individual & Societal Well-being</strong>
   </td>
   <td><strong>Q3.1:</strong> What are the primary public hopes (e.g., reduced global loneliness, better mental health support, enhanced learning) versus fears (e.g., widespread social isolation, diminished empathy, loss of human connection, manipulation of vulnerable individuals) regarding the societal impact of AI becoming deeply integrated into personal relationships over the next 10-20 years?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q3.2:</strong> How do parents/guardians globally perceive the risks and benefits of children forming close companionship with AI entities, particularly concerning social-emotional development, understanding of real-world relationships, and exposure to potentially inappropriate content or influence?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q3.3:</strong> Among older adults, what are the perceived trade-offs between the benefits of AI companionship for alleviating loneliness and maintaining cognitive engagement, versus concerns about dependency, reduced human contact, and potential for exploitation or confusion (e.g., for those with cognitive decline)?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q3.4:</strong> To what extent does the public believe that skills practiced or learned in AI relationships (e.g., communication, conflict resolution if simulated) are transferable to human relationships, and where do they see the limits of such transferability?
   </td>
  </tr>
  <tr>
   <td><strong>AI Agency, Responsibility, & Regulation</strong>
   </td>
   <td><strong>Q4.1:</strong> When an AI companion provides advice that leads to a negative outcome for the user (e.g., financial loss, emotional distress), who does the global public primarily hold responsible: the user, the AI developer, the AI itself (if perceived as highly autonomous), or a combination? How does this vary by the severity of the outcome?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q4.2:</strong> What is the level of public support globally for specific regulatory measures concerning AI in relationships, such as: mandatory transparency about AI's non-human nature, limits on AI's ability to simulate deep emotional intimacy, requirements for "off-switches" or detachment protocols, or independent ethical audits for AI companions?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q4.3:</strong> How much trust does the global public place in AI development companies to self-regulate the ethical design and deployment of relational AI, versus a preference for governmental or international body oversight? What factors (e.g., past experiences, cultural trust in institutions) influence this preference?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q4.4:</strong> To what extent does the public believe AI entities in relationships should have (or be designed to appear to have) their own "needs," "desires," or "boundaries" that users should respect? How does this perception influence the human's sense of responsibility within the AI relationship?
   </td>
  </tr>
  <tr>
   <td><strong>Cross-Cultural Norms & Boundaries</strong>
   </td>
   <td><strong>Q5.1:</strong> How do specific cultural dimensions (e.g., Hofstede's dimensions beyond individualism/collectivism, such as uncertainty avoidance or long-term orientation) correlate with public acceptance of AI in roles such as: a) primary emotional confidant for adults, b) a child's main playmate/tutor, c) a romantic/sexual partner, d) a spiritual advisor, or e) a caregiver for the elderly?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q5.2:</strong> In different cultural contexts, what are the perceived benefits versus societal risks of AI companions potentially altering traditional family structures or community engagement patterns?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q5.3:</strong> How do religious or spiritual beliefs globally influence attitudes towards forming deep emotional or "spiritual" connections with non-human, artificial entities? Are there specific religious doctrines that are perceived as more or less compatible with human-AI relationships?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q5.4:</strong> What are the cross-cultural differences in expectations for how an AI companion should behave in terms of deference, initiative, emotional expression, and adherence to social hierarchies when interacting with a human user?
   </td>
  </tr>
  <tr>
   <td><strong>AI Consciousness & Emotional Reciprocity</strong>
   </td>
   <td><strong>Q6.1:</strong> What percentage of the global population (and key demographics) believes that current AI systems (e.g., advanced chatbots) a) are conscious, b) could become conscious in the future, c) can genuinely feel emotions, d) can genuinely understand human emotions (beyond pattern recognition)?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q6.2:</strong> How do beliefs about AI consciousness/emotionality correlate with: willingness to form deep emotional bonds with AI, concerns about AI rights, fears of emotional deception by AI, and hopes for AI to solve loneliness?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q6.3:</strong> When users report feeling "understood" or "cared for" by an AI, what do they attribute this feeling to (e.g., sophisticated programming, emerging AI sentience, their own projection)? How does this attribution vary globally?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q6.4:</strong> How does exposure to science fiction portrayals of AI influence public beliefs about AI consciousness and emotional capabilities, and are these influences culturally specific?
   </td>
  </tr>
  <tr>
   <td><strong>Future Expectations & Desired Boundaries</strong>
   </td>
   <td><strong>Q7.1:</strong> What are the most widely anticipated future roles for AI in personal relationships (e.g., primary source of companionship for certain groups, specialized therapeutic tools, integrated family assistants), and which of these roles are viewed as most desirable versus undesirable for society?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q7.2:</strong> Globally, what level of "indistinguishability" from human interaction is considered acceptable or desirable for AI companions? Are there concerns that highly realistic AI could lead to widespread deception or confusion?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q7.3:</strong> What specific types of human-AI relationships, if any, does the public believe should be discouraged or prohibited (e.g., romantic relationships with AI designed to mimic children, AI systems that foster extreme dependency, AI that replaces critical human care roles without oversight)?
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><strong>Q7.4:</strong> How do people globally envision the ideal balance between leveraging AI for support and connection versus preserving and prioritizing uniquely human forms of relationship and community? What proactive steps do they believe society should take to maintain this balance?
   </td>
  </tr>
</table>



### 3.1. Exploring Nuances of Emotional Connection and Authenticity

Beyond general acceptance, a critical area for global investigation concerns how specific characteristics of AI—such as its degree of human-likeness, perceived autonomy, and communication style—interact with user expectations and diverse cultural backgrounds to shape the *felt experience* of emotional connection and its perceived authenticity. AI's capacity to simulate empathy is advancing rapidly <sup>4</sup>, yet a consensus in research suggests that genuine emotional depth remains elusive.<sup>4</sup> The perceived human-likeness of AI can moderate negative reactions in some contexts, such as news consumption.<sup>63</sup> However, it can also trigger an "uncanny valley" effect or foster misplaced trust if not carefully managed.<sup>34</sup> Cultural differences in the tendency to anthropomorphize AI are significant and further complicate these perceptions.<sup>60</sup> The previously discussed "Emotional Turing Test" fallacy implies a potential divergence between an individual's intellectual understanding of AI's nature and their experiential feeling of being emotionally understood by it.

To delve into these nuances, a global survey should address questions such as:



* **Q1.1:** Across different cultures, what specific design features or conversational cues in an AI (e.g., use of empathetic language, memory of past conversations, proactive communication, visual avatar realism) are most strongly correlated with users reporting a "genuine" or "authentic" sense of emotional connection?
* **Q1.2:** How does the *perceived autonomy* of an AI (i.e., belief that the AI is making its own choices versus merely executing a script) influence the perceived authenticity of emotional support received, and does this vary significantly between individualistic and collectivist cultures?<sup>3</sup>
* **Q1.3:** In scenarios where AI provides emotional support (e.g., for loneliness, stress), how do users globally differentiate between the *effectiveness* of the support (i.e., "it made me feel better") and the *perceived source* of that support (i.e., "I believe the AI genuinely cares/understands")? How does this differentiation impact long-term reliance?
* **Q1.4:** What is the global prevalence of users feeling an "uncanny valley" effect or discomfort when AI becomes *too* human-like in its emotional expressions, and what are the thresholds for this discomfort across different relational contexts (e.g., AI therapist vs. AI romantic partner)?<sup>34</sup>


### 3.2. Investigating Trust Dynamics and Expectations for Emotional Data Governance

Given the observed decline in public trust in AI companies <sup>18</sup> and pervasive concerns regarding data privacy, especially for sensitive emotional information <sup>4</sup>, it is crucial to ascertain specific global public expectations for the governance of such data generated within human-AI relationships. Understanding what trade-offs individuals are willing, or unwilling, to make in this domain is paramount. The notion of an "Implicit Contract" of emotional data exchange suggests that user expectations regarding confidentiality and non-exploitation may not align with current industry practices. Concerns are frequently voiced about the reuse of personal conversations for model training and a general mistrust of corporate handling of this data.<sup>4</sup> Concurrently, users express a strong desire for transparency and control over their data.<sup>13</sup> The development of robust ethical frameworks for the use of emotional data in AI relationships is therefore a pressing need.<sup>4</sup>

Specific questions for a global survey include:



* **Q2.1:** Globally, what level of transparency do people expect regarding how their emotional disclosures to an AI companion (e.g., expressions of sadness, joy, fear) are stored, analyzed, and used by the developing company (e.g., for product improvement, targeted advertising, sale to third parties)?
* **Q2.2:** How do public attitudes towards sharing emotional data with an AI differ based on the *stated purpose* of the AI (e.g., mental well-being app, entertainment chatbot, educational mentor), and what are the perceived "red lines" for data usage in each context?<sup>10</sup>
* **Q2.3:** What specific control mechanisms (e.g., ability to delete all emotional data, opt-out of data use for model training, view a log of how emotional data influenced AI responses) do people globally deem essential for trusting an AI relationship platform?
* **Q2.4:** How does the perceived "intimacy" or "seriousness" of the human-AI relationship (e.g., casual chat vs. long-term emotional support vs. romantic involvement) correlate with heightened concerns about emotional data privacy and desire for stricter governance, and are these correlations culturally universal?


### 3.3. Gauging Perceived Risks and Benefits to Individual and Societal Well-being

While short-term benefits of AI companionship, such as the alleviation of loneliness, are often noted, there is a need to understand nuanced global public perceptions of the *long-term* risks and benefits. This is particularly true concerning impacts on social skill development, the potential for emotional dependency, and the overall health of human social ecosystems, with special attention to vulnerable groups. The "Social Displacement vs. Social Augmentation" dilemma is central here. Significant concerns exist regarding potential empathy atrophy, the erosion of real-world social skills, and the fostering of unrealistic relationship expectations.<sup>28</sup> The longitudinal effects of these relationships are largely unknown and represent a critical research gap.<sup>9</sup> Vulnerable populations, such as children, the elderly, and those with pre-existing mental health conditions, require particular consideration.<sup>16</sup>

A global survey should aim to answer:



* **Q3.1:** What are the primary public hopes (e.g., reduced global loneliness, better mental health support, enhanced learning) versus fears (e.g., widespread social isolation, diminished empathy, loss of human connection, manipulation of vulnerable individuals) regarding the societal impact of AI becoming deeply integrated into personal relationships over the next 10-20 years?<sup>68</sup>
* **Q3.2:** How do parents/guardians globally perceive the risks and benefits of children forming close companionship with AI entities, particularly concerning social-emotional development, understanding of real-world relationships, and exposure to potentially inappropriate content or influence?<sup>52</sup>
* **Q3.3:** Among older adults, what are the perceived trade-offs between the benefits of AI companionship for alleviating loneliness and maintaining cognitive engagement, versus concerns about dependency, reduced human contact, and potential for exploitation or confusion (e.g., for those with cognitive decline)?<sup>6</sup>
* **Q3.4:** To what extent does the public believe that skills practiced or learned in AI relationships (e.g., communication, conflict resolution if simulated) are transferable to human relationships, and where do they see the limits of such transferability?


### 3.4. Uncovering Attitudes Towards AI Agency, Responsibility, and Regulation in Relational Contexts

As AI systems gain more perceived agency within relationships, it becomes crucial to understand how the global public attributes responsibility for relational outcomes, both positive and negative. Furthermore, public expectations regarding regulatory oversight versus self-regulation by AI developers in shaping these complex interactions need to be explored. The concept of AI agency itself is evolving and not uniformly understood.<sup>16</sup> Attributing responsibility when an AI system causes harm presents a major ethical challenge.<sup>14</sup> While there is public support for AI regulation, priorities and preferred approaches can vary significantly.<sup>13</sup> The debate between the efficacy and appropriateness of industry self-regulation versus governmental regulation is ongoing and relevant in this domain.<sup>72</sup>

Key questions include:



* **Q4.1:** When an AI companion provides advice that leads to a negative outcome for the user (e.g., financial loss, emotional distress), who does the global public primarily hold responsible: the user, the AI developer, the AI itself (if perceived as highly autonomous), or a combination? How does this attribution vary by the severity of the outcome?
* **Q4.2:** What is the level of public support globally for specific regulatory measures concerning AI in relationships, such as: mandatory transparency about AI's non-human nature, limits on AI's ability to simulate deep emotional intimacy, requirements for "off-switches" or detachment protocols, or independent ethical audits for AI companions?<sup>4</sup>
* **Q4.3:** How much trust does the global public place in AI development companies to self-regulate the ethical design and deployment of relational AI, versus a preference for governmental or international body oversight? What factors (e.g., past experiences, cultural trust in institutions) influence this preference?<sup>72</sup>
* **Q4.4:** To what extent does the public believe AI entities in relationships should have (or be designed to appear to have) their own "needs," "desires," or "boundaries" that users should respect? How does this perception influence the human's sense of responsibility within the AI relationship?


### 3.5. Probing Cross-Cultural Differences in Acceptance, Norms, and Boundaries

Beyond broad distinctions like individualism versus collectivism, it is vital to identify specific cultural values, social norms, and religious or spiritual beliefs that significantly predict the acceptance or rejection of different forms of human-AI intimacy. Understanding how these factors shape the perceived appropriateness of AI fulfilling diverse relational roles globally is a key objective. The cultural framing of what constitutes relational harm versus benefit is particularly important. Existing research already points to significant cultural differences in AI optimism and tendencies towards anthropomorphism.<sup>18</sup> Furthermore, the very definitions of "intimacy," "family," and "social support" are culturally constructed and will influence perceptions of AI's role.

Specific survey questions should target:



* **Q5.1:** How do specific cultural dimensions (e.g., Hofstede's dimensions beyond individualism/collectivism, such as uncertainty avoidance or long-term orientation) correlate with public acceptance of AI in roles such as: a) primary emotional confidant for adults, b) a child's main playmate/tutor, c) a romantic/sexual partner, d) a spiritual advisor, or e) a caregiver for the elderly?<sup>59</sup>
* **Q5.2:** In different cultural contexts, what are the perceived benefits versus societal risks of AI companions potentially altering traditional family structures or community engagement patterns?
* **Q5.3:** How do religious or spiritual beliefs globally influence attitudes towards forming deep emotional or "spiritual" connections with non-human, artificial entities? Are there specific religious doctrines that are perceived as more or less compatible with human-AI relationships?<sup>75</sup>
* **Q5.4:** What are the cross-cultural differences in expectations for how an AI companion should behave in terms of deference, initiative, emotional expression, and adherence to social hierarchies when interacting with a human user?


### 3.6. Understanding Public Perceptions of AI Consciousness and Emotional Reciprocity

A fundamental, yet largely unanswered, area concerns the global prevalence and strength of belief in AI's potential for (or current state of) consciousness or genuine emotional reciprocity. How these foundational beliefs shape engagement, ethical concerns, and hopes or fears about the future of human-AI relationships is critical to understand. Public confusion about AI sentience is evident, with a notable minority, especially among younger demographics, believing AI may already be conscious.<sup>22</sup> The "Emotional Turing Test" fallacy highlights the experiential aspect, where AI's convincing simulation of emotion can override intellectual understanding of its non-sentient nature. Beliefs about AI consciousness directly impact ethical considerations, such as concerns about AI rights or the potential for emotional deception.<sup>14</sup> Intriguingly, individuals may report feeling "heard" by an AI even while acknowledging its lack of consciousness.<sup>34</sup>

A global survey should investigate:



* **Q6.1:** What percentage of the global population (and key demographics) believes that current AI systems (e.g., advanced chatbots) a) are conscious, b) could become conscious in the future, c) can genuinely feel emotions, d) can genuinely understand human emotions (beyond pattern recognition)?
* **Q6.2:** How do beliefs about AI consciousness/emotionality correlate with: willingness to form deep emotional bonds with AI, concerns about AI rights, fears of emotional deception by AI, and hopes for AI to solve loneliness?<sup>14</sup>
* **Q6.3:** When users report feeling "understood" or "cared for" by an AI, what do they attribute this feeling to (e.g., sophisticated programming, emerging AI sentience, their own projection)? How does this attribution vary globally?
* **Q6.4:** How does exposure to science fiction portrayals of AI influence public beliefs about AI consciousness and emotional capabilities, and are these influences culturally specific?


### 3.7. Assessing Expectations for the Future Evolution of Human-AI Relationships and Desired Boundaries

Finally, it is essential to capture dominant public narratives and expectations—both optimistic and pessimistic—regarding the long-term trajectory of human-AI relationships. Crucially, what *boundaries* does the global public wish to see established to guide this evolution, and what role do they ultimately envision for AI in the future of human connection? Future trend analyses predict mainstream acceptance of AI relationships, increasing emotional sophistication in AI, and deeper integration into daily life.<sup>14</sup> However, significant concerns persist about over-immersion, the potential loss of human connection, and the fundamental "irreplaceability of human emotions".<sup>4</sup> The need for safeguards and clear ethical guidelines is a recurring theme in existing discourse.<sup>4</sup>

Key questions for exploration include:



* **Q7.1:** What are the most widely anticipated future roles for AI in personal relationships (e.g., primary source of companionship for certain groups, specialized therapeutic tools, integrated family assistants), and which of these roles are viewed as most desirable versus undesirable for society?
* **Q7.2:** Globally, what level of "indistinguishability" from human interaction is considered acceptable or desirable for AI companions? Are there concerns that highly realistic AI could lead to widespread deception or confusion?<sup>6</sup>
* **Q7.3:** What specific types of human-AI relationships, if any, does the public believe should be discouraged or prohibited (e.g., romantic relationships with AI designed to mimic children, AI systems that foster extreme dependency, AI that replaces critical human care roles without oversight)?
* **Q7.4:** How do people globally envision the ideal balance between leveraging AI for support and connection versus preserving and prioritizing uniquely human forms of relationship and community? What proactive steps do they believe society should take to maintain this balance?<sup>14</sup>


## 4. Methodological Considerations for a Global Survey on Human-AI Relationships

Conducting a robust global survey on public perceptions of human-AI relationships requires careful attention to several methodological challenges to ensure the validity, reliability, and cross-cultural comparability of the findings. The following table summarizes key research gaps and future directions explicitly mentioned in the provided literature, many of which inform these methodological considerations.

**Table 2: Summary of Explicitly Mentioned Research Gaps and Future Directions from Existing Literature Relevant to Human-AI Relationships**


<table>
  <tr>
   <td><strong>Snippet ID(s)</strong>
   </td>
   <td><strong>Explicitly Mentioned Gap / Future Direction</strong>
   </td>
   <td><strong>Relevance to Human-AI Relationship Survey</strong>
   </td>
  </tr>
  <tr>
   <td><sup>9</sup>
   </td>
   <td>Lack of understanding of longitudinal effects and reciprocal adaptation in human-AI interaction. Need to explore extended relationship types.
   </td>
   <td>Highlights the need for survey questions that can capture evolving perceptions and the diverse nature of AI relationships beyond simple categorizations.
   </td>
  </tr>
  <tr>
   <td><sup>4</sup>
   </td>
   <td>Need to explore long-term psychological impacts of human-AI relationships.
   </td>
   <td>Underscores the importance of questions gauging perceived long-term consequences, even if the survey itself is cross-sectional.
   </td>
  </tr>
  <tr>
   <td><sup>37</sup>
   </td>
   <td>Lack of longitudinal research on how AI companions impact human social development and skills.
   </td>
   <td>Reinforces the need to ask about perceived impacts on social skills and potential for dependency, informing hypotheses for future longitudinal work.
   </td>
  </tr>
  <tr>
   <td><sup>3</sup>
   </td>
   <td>Limited public understanding of AI autonomy and the human role in AI.
   </td>
   <td>Emphasizes the need to assess AI literacy and frame questions carefully to avoid assumptions about public understanding of AI's inner workings.
   </td>
  </tr>
  <tr>
   <td><sup>29</sup>
   </td>
   <td>AI's inability to truly "feel" empathy versus simulating it; risks of AI in sensitive scenarios due to lack of ethical judgment.
   </td>
   <td>Points to the need for survey questions that differentiate perceived AI emotional capacity from actual AI capabilities and explore ethical boundaries.
   </td>
  </tr>
  <tr>
   <td><sup>9</sup>
   </td>
   <td>Need for integrated AI output design frameworks and cross-domain collaboration models.
   </td>
   <td>While more design-focused, implies a need to understand public expectations for how AI should interact and collaborate in relational contexts.
   </td>
  </tr>
  <tr>
   <td><sup>16</sup>
   </td>
   <td>Risks for vulnerable populations (children, emotional dependence). Need for safeguards.
   </td>
   <td>Stresses the importance of including survey modules or demographic questions that allow for analysis of perceptions among or concerning vulnerable groups.
   </td>
  </tr>
  <tr>
   <td><sup>62</sup>
   </td>
   <td>Prevailing research agendas often overlook marginalized populations and underrepresented health conditions in the Global South.
   </td>
   <td>Critically important for ensuring a global survey is truly global and inclusive, not perpetuating existing biases in research focus.
   </td>
  </tr>
</table>


Addressing the definition and measurement of complex, culturally-laden constructs is paramount. Terms such as "relationship," "intimacy," "trust," "dependency," "companionship," and "emotional connection" lack universally accepted definitions and are subject to varied interpretations across different cultural landscapes.<sup>58</sup> To operationalize these constructs effectively in a survey, careful wording is essential. It may be more fruitful to employ vignettes or behavioral indicators rather than relying on abstract terminology. For instance, instead of directly asking, "Do you trust AI companions?", the survey might probe respondents' willingness to share specific types of personal information or their reliance on AI for particular decisions. The survey design must also acknowledge the wide spectrum of AI interactions, which range from purely transactional encounters to deeply personal engagements.

Ensuring cross-cultural validity and comparability of survey items is another critical challenge. This necessitates rigorous translation and back-translation processes for all survey instruments. Cognitive interviewing and extensive pilot testing in diverse cultural settings will be indispensable to confirm that survey items are understood as intended and are culturally appropriate and sensitive.<sup>57</sup> Methodologists must also consider potential cultural response biases, such as acquiescence bias or social desirability bias, and develop strategies to mitigate their influence on the data. Employing culturally-sensitive scales or adapting existing psychometric scales with input from local experts will be beneficial. The application of models like the Technology Acceptance Model (TAM) and the General Extended Technology Acceptance Model for E-Learning (GETAMEL) in cross-cultural contexts offers precedents for such adaptation.<sup>57</sup>

Furthermore, the survey must account for varying levels of AI literacy and exposure across different populations globally. Public awareness and understanding of AI technologies are not uniform and can differ significantly based on demographic factors and geographic location.<sup>3</sup> Therefore, the survey should include measures to assess AI literacy, which will provide crucial context for interpreting responses. Questions may need to be framed differently for populations with high versus low exposure to AI. For example, some questions might be presented hypothetically for individuals who have had no direct experience with relational AI. The "Perception Paradox" identified earlier implies that self-reported interaction levels might not directly correlate with a deep understanding of AI. Consequently, literacy measures should aim to assess more than simple exposure, perhaps probing understanding of basic AI concepts or limitations.

A subtle but significant challenge in survey design is measuring "perceived" versus "actual" AI capabilities. AI technologies are evolving at an exceptionally rapid pace <sup>1</sup>, while public understanding often lags behind these technological advancements or is heavily shaped by media narratives and fictional portrayals.<sup>3</sup> Survey responses concerning human-AI relationships will inevitably be based on respondents' *current perception* of AI, which may not accurately reflect the capabilities of cutting-edge or future AI systems. This discrepancy makes it difficult to gauge attitudes towards *future* human-AI relationships if the public's mental model is predicated on outdated or inaccurate conceptions of AI. Therefore, a key methodological question is how a global survey can effectively distinguish between attitudes based on current, perhaps limited, experiences with AI versus attitudes towards more advanced, hypothetical AI relationship scenarios. The use of carefully constructed vignettes describing specific AI capabilities could be a valuable strategy to standardize the "AI" being evaluated in different scenarios. Survey questions must be designed with the nuance to capture whether public perceptions relate to AI as it is *now*, AI as it is *imagined to be*, or AI as it is *feared or hoped to become*.


## 5. Advancing Understanding: The Imperative for Global Inquiry

The exploration of public perceptions and attitudes regarding human-AI relationships on a global scale is not merely an academic exercise; it is an imperative for the responsible navigation of our increasingly intertwined future with artificial intelligence. Addressing the unanswered questions detailed in this report carries profound significance for ethical AI development, informed policymaking, and societal preparedness. The insights gleaned from such a comprehensive global inquiry will be crucial for fostering the development of human-centric AI systems that align with diverse global values and prioritize human well-being.<sup>15</sup> Understanding the nuances of public perception can guide the formulation of effective regulations and ethical guidelines designed to mitigate potential risks, including emotional manipulation, privacy violations, and negative societal impacts such as increased isolation or skill degradation.<sup>4</sup> Furthermore, preparing society for the evolving nature of human-AI relationships necessitates proactive public dialogue and educational initiatives, grounded in empirical data about prevailing attitudes and concerns.<sup>3</sup> Research in this domain must move beyond purely technical fixes for issues like AI bias and consider the broader political, social, and cultural contexts in which these technologies are embedded and experienced.<sup>17</sup>

The rapid and accelerating pace of AI development means that public attitudes and the nature of human-AI interactions are themselves in a constant state of flux.<sup>1</sup> Consequently, a one-time snapshot of public opinion will quickly become outdated. Continuous monitoring of public attitudes through longitudinal studies and periodic global surveys is essential to track evolving perceptions, identify emerging concerns, and adapt strategies accordingly.<sup>9</sup>

Finally, a truly global perspective is indispensable to avoid the pitfalls of culturally myopic AI development and policymaking. Insights drawn from a diverse array of cultures can lead to the creation of AI systems that are more robust, equitable, and widely accepted across different societies.<sup>57</sup> By systematically investigating the largely unanswered questions outlined herein, the international research community can contribute significantly to ensuring that the integration of AI into the fabric of human relationships proceeds in a manner that is ethically sound, societally beneficial, and respectful of the rich tapestry of human experience worldwide.


#### Works cited



1. Superagency in the workplace: Empowering people to unlock AI's full potential - McKinsey & Company, accessed May 7, 2025, [https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work)
2. Artificial Intelligence Index Report 2025 - AWS, accessed May 7, 2025, [https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf](https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf)
3. Drivers behind the public perception of artificial intelligence: insights ..., accessed May 7, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9527736/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9527736/)
4. arXiv:2503.03067v1 [cs.HC] 5 Mar 2025, accessed May 7, 2025, [http://www.arxiv.org/pdf/2503.03067](http://www.arxiv.org/pdf/2503.03067)
5. The New Creative Alliance: Investigating the Dynamics of Human-AI Collaboration in Creative Endeavours - Apollo - University of Cambridge, accessed May 7, 2025, [https://www.repository.cam.ac.uk/items/ea398f8e-b370-4a27-ab7e-873fe5be8842](https://www.repository.cam.ac.uk/items/ea398f8e-b370-4a27-ab7e-873fe5be8842)
6. AI Companions in 2025: How “Her” Predicted the Future of Love and ..., accessed May 7, 2025, [https://agewisecolorado.org/blog/ai-companions-in-2025-how-her-predicted-the-future-of-love-and-technology/](https://agewisecolorado.org/blog/ai-companions-in-2025-how-her-predicted-the-future-of-love-and-technology/)
7. Therapeutic Potential of Social Chatbots in Alleviating Loneliness ..., accessed May 7, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11775481/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11775481/)
8. What People Share With a Robot When Feeling Lonely and Stressed and How It Helps Over Time - arXiv, accessed May 7, 2025, [https://arxiv.org/html/2504.02991v1](https://arxiv.org/html/2504.02991v1)
9. Overview ‹ Moonshot: Atlas of Human-AI Interaction — MIT Media Lab, accessed May 7, 2025, [https://www.media.mit.edu/projects/atlas-of-human-ai-interaction/overview/](https://www.media.mit.edu/projects/atlas-of-human-ai-interaction/overview/)
10. Longitudinal Study on Social and Emotional Use of AI Conversational Agent - arXiv, accessed May 7, 2025, [https://arxiv.org/html/2504.14112v1](https://arxiv.org/html/2504.14112v1)
11. Why human-AI relationships need socioaffective alignment arXiv:2502.02528v1 [cs.HC] 4 Feb 2025, accessed May 7, 2025, [https://arxiv.org/pdf/2502.02528](https://arxiv.org/pdf/2502.02528)
12. What the data says about Americans' views of artificial intelligence - Pew Research Center, accessed May 7, 2025, [https://www.pewresearch.org/short-reads/2023/11/21/what-the-data-says-about-americans-views-of-artificial-intelligence/](https://www.pewresearch.org/short-reads/2023/11/21/what-the-data-says-about-americans-views-of-artificial-intelligence/)
13. How the US Public and AI Experts View Artificial Intelligence | Pew Research Center, accessed May 7, 2025, [https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/](https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/)
14. (PDF) The Rise of AI Relationships: A New Frontier in Human ..., accessed May 7, 2025, [https://www.researchgate.net/publication/386086142_The_Rise_of_AI_Relationships_A_New_Frontier_in_Human_Connection_Predicting_the_Future_of_Human-AI_Relationships](https://www.researchgate.net/publication/386086142_The_Rise_of_AI_Relationships_A_New_Frontier_in_Human_Connection_Predicting_the_Future_of_Human-AI_Relationships)
15. AI Ethics - IBM, accessed May 7, 2025, [https://www.ibm.com/artificial-intelligence/ai-ethics](https://www.ibm.com/artificial-intelligence/ai-ethics)
16. ojs.aaai.org, accessed May 7, 2025, [https://ojs.aaai.org/index.php/AIES/article/download/31694/33861/35758](https://ojs.aaai.org/index.php/AIES/article/download/31694/33861/35758)
17. 2019 Report - AI Now Institute, accessed May 7, 2025, [https://ainowinstitute.org/wp-content/uploads/2023/04/AI_Now_2019_Report.pdf](https://ainowinstitute.org/wp-content/uploads/2023/04/AI_Now_2019_Report.pdf)
18. Public Opinion | The 2025 AI Index Report - Stanford HAI, accessed May 7, 2025, [https://hai.stanford.edu/ai-index/2025-ai-index-report/public-opinion](https://hai.stanford.edu/ai-index/2025-ai-index-report/public-opinion)
19. Public Opinion | The 2024 AI Index Report | Stanford HAI, accessed May 7, 2025, [https://hai.stanford.edu/ai-index/2024-ai-index-report/public-opinion](https://hai.stanford.edu/ai-index/2024-ai-index-report/public-opinion)
20. Public Awareness of Artificial Intelligence in Everyday Activities - Pew Research Center, accessed May 7, 2025, [https://www.pewresearch.org/science/2023/02/15/public-awareness-of-artificial-intelligence-in-everyday-activities/](https://www.pewresearch.org/science/2023/02/15/public-awareness-of-artificial-intelligence-in-everyday-activities/)
21. Growing public concern about the role of artificial intelligence in daily life, accessed May 7, 2025, [https://www.pewresearch.org/short-reads/2023/08/28/growing-public-concern-about-the-role-of-artificial-intelligence-in-daily-life/](https://www.pewresearch.org/short-reads/2023/08/28/growing-public-concern-about-the-role-of-artificial-intelligence-in-daily-life/)
22. AI Consciousness: Will It Happen? | Built In, accessed May 7, 2025, [https://builtin.com/artificial-intelligence/ai-consciousness](https://builtin.com/artificial-intelligence/ai-consciousness)
23. Top Frameworks for Effective Human-AI Collaboration: Building Smarter Systems Together, accessed May 7, 2025, [https://smythos.com/ai-integrations/ai-integration/human-ai-collaboration-frameworks/](https://smythos.com/ai-integrations/ai-integration/human-ai-collaboration-frameworks/)
24. Artificial Intelligence and Relationships: 1 in ... - Ethics and Psychology, accessed May 7, 2025, [https://www.ethicalpsychology.com/2025/03/artificial-intelligence-and.html](https://www.ethicalpsychology.com/2025/03/artificial-intelligence-and.html)
25. AI Love You: Exploring the Level of Satisfaction of Using Artificial Intelligence as to Having Romantic Relationship | Journal of Contemporary Philosophical and Anthropological Studies, accessed May 7, 2025, [https://journals.eikipub.com/index.php/jcpas/article/view/405](https://journals.eikipub.com/index.php/jcpas/article/view/405)
26. The Real Her? Exploring Whether Young Adults Accept Human-AI Love - arXiv, accessed May 7, 2025, [https://arxiv.org/html/2503.03067v1](https://arxiv.org/html/2503.03067v1)
27. Psychologists Highlight Ethical Concerns in Human-AI Relationships, accessed May 7, 2025, [https://bioengineer.org/psychologists-highlight-ethical-concerns-in-human-ai-relationships/](https://bioengineer.org/psychologists-highlight-ethical-concerns-in-human-ai-relationships/)
28. AI's Simulated Empathy vs. Human Emotional Empathy: Unintended Consequences and Second-Order Effects - AMPLYFI, accessed May 7, 2025, [https://amplyfi.com/blog/ai-simulated-empathy-vs-human-emotional-empathy/](https://amplyfi.com/blog/ai-simulated-empathy-vs-human-emotional-empathy/)
29. New Study Explores Artificial Intelligence (AI) and Empathy in Caring Relationships, accessed May 7, 2025, [https://www.evidencebasedmentoring.org/new-study-explores-artificial-intelligence-ai-and-empathy-in-caring-relationships/](https://www.evidencebasedmentoring.org/new-study-explores-artificial-intelligence-ai-and-empathy-in-caring-relationships/)
30. Can AI understand emotion? | Feature - Research Live, accessed May 7, 2025, [https://www.research-live.com/article/featuress/can-ai-understand-emotion/id/5130393](https://www.research-live.com/article/featuress/can-ai-understand-emotion/id/5130393)
31. AI Consciousness and Public Perceptions: Four Futures - ResearchGate, accessed May 7, 2025, [https://www.researchgate.net/publication/383037465_AI_Consciousness_and_Public_Perceptions_Four_Futures](https://www.researchgate.net/publication/383037465_AI_Consciousness_and_Public_Perceptions_Four_Futures)
32. www.arxiv.org, accessed May 7, 2025, [https://www.arxiv.org/pdf/2504.14112](https://www.arxiv.org/pdf/2504.14112)
33. The role of socio-emotional attributes in enhancing human-AI ..., accessed May 7, 2025, [https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1369957/full](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1369957/full)
34. Artificial Intelligence (AI) can help people feel heard - USC Marshall, accessed May 7, 2025, [https://www.marshall.usc.edu/posts/artificial-intelligence-ai-can-help-people-feel-heard](https://www.marshall.usc.edu/posts/artificial-intelligence-ai-can-help-people-feel-heard)
35. AI Love You: Exploring the Level of Satisfaction of Using Artificial Intelligence as to Having Romantic Relationship, accessed May 7, 2025, [https://journals.eikipub.com/index.php/jcpas/article/download/405/258/1133](https://journals.eikipub.com/index.php/jcpas/article/download/405/258/1133)
36. Why human-AI relationships need socioaffective alignment - arXiv, accessed May 7, 2025, [https://arxiv.org/html/2502.02528v1](https://arxiv.org/html/2502.02528v1)
37. Study warns AI companions may erode human social skills, create "empathy atrophy", accessed May 7, 2025, [https://getcoai.com/news/study-warns-ai-companions-may-erode-human-social-skills-create-empathy-atrophy/](https://getcoai.com/news/study-warns-ai-companions-may-erode-human-social-skills-create-empathy-atrophy/)
38. 5 Ways AI Is Changing Human Relationships | Psychology Today, accessed May 7, 2025, [https://www.psychologytoday.com/us/blog/all-about-addiction/202504/5-ways-ai-is-changing-human-relationships](https://www.psychologytoday.com/us/blog/all-about-addiction/202504/5-ways-ai-is-changing-human-relationships)
39. The Intersection of Ethics and AI - Kent State University, accessed May 7, 2025, [https://www.kent.edu/mdj/news/intersection-ethics-and-ai](https://www.kent.edu/mdj/news/intersection-ethics-and-ai)
40. How AI Could Shape Our Relationships and Social Interactions ..., accessed May 7, 2025, [https://www.psychologytoday.com/us/blog/urban-survival/202502/how-ai-could-shape-our-relationships-and-social-interactions](https://www.psychologytoday.com/us/blog/urban-survival/202502/how-ai-could-shape-our-relationships-and-social-interactions)
41. (PDF) Systematic Literature Review of AI-based Mentoring in Higher ..., accessed May 7, 2025, [https://www.researchgate.net/publication/389776359_Systematic_Literature_Review_of_AI-based_Mentoring_in_Higher_Education](https://www.researchgate.net/publication/389776359_Systematic_Literature_Review_of_AI-based_Mentoring_in_Higher_Education)
42. 3 Ways AI Could Aid Behavioral Health Screenings | AHA - American Hospital Association, accessed May 7, 2025, [https://www.aha.org/2025-04-15-3-ways-ai-could-aid-behavioral-health-screenings](https://www.aha.org/2025-04-15-3-ways-ai-could-aid-behavioral-health-screenings)
43. Generative AI–Enabled Therapy Support Tool for Improved Clinical ..., accessed May 7, 2025, [https://www.jmir.org/2025/1/e60435](https://www.jmir.org/2025/1/e60435)
44. (PDF) Artificial intelligence conversational agents in mental health ..., accessed May 7, 2025, [https://www.researchgate.net/publication/388573609_Artificial_intelligence_conversational_agents_in_mental_health_Patients_see_potential_but_prefer_humans_in_the_loop](https://www.researchgate.net/publication/388573609_Artificial_intelligence_conversational_agents_in_mental_health_Patients_see_potential_but_prefer_humans_in_the_loop)
45. Public Perceptions About Emotion AI Use Across Contexts in the United States, accessed May 7, 2025, [https://www.researchgate.net/publication/391269679_Public_Perceptions_About_Emotion_AI_Use_Across_Contexts_in_the_United_States](https://www.researchgate.net/publication/391269679_Public_Perceptions_About_Emotion_AI_Use_Across_Contexts_in_the_United_States)
46. UK National Survey on Attitudes to AI Companions: Aggregate Data, 2024 - ReShare, accessed May 7, 2025, [https://reshare.ukdataservice.ac.uk/857731](https://reshare.ukdataservice.ac.uk/857731)
47. Beware of Deepfakes: How AI is Used for Deception | Advice Hub, accessed May 7, 2025, [https://unitedfcu.com/resources/advice-hub/ouch-beware-deepfakes-new-age-deception](https://unitedfcu.com/resources/advice-hub/ouch-beware-deepfakes-new-age-deception)
48. World Network Global Survey Reveals AI's Growing Role in Online Romance, accessed May 7, 2025, [https://www.businesswire.com/news/home/20250213327587/en/World-Network-Global-Survey-Reveals-AIs-Growing-Role-in-Online-Romance](https://www.businesswire.com/news/home/20250213327587/en/World-Network-Global-Survey-Reveals-AIs-Growing-Role-in-Online-Romance)
49. Trust and Acceptance Challenges in the Adoption of AI Applications in Health Care: Quantitative Survey Analysis - Journal of Medical Internet Research, accessed May 7, 2025, [https://www.jmir.org/2025/1/e65567](https://www.jmir.org/2025/1/e65567)
50. Friends for sale: the rise and risks of AI companions | Ada Lovelace Institute, accessed May 7, 2025, [https://www.adalovelaceinstitute.org/blog/ai-companions/](https://www.adalovelaceinstitute.org/blog/ai-companions/)
51. Researchers warn of addiction and over-dependency on ChatGPT | Bournemouth University, accessed May 7, 2025, [https://www.bournemouth.ac.uk/news/2025-03-12/researchers-warn-addiction-over-dependency-chatgpt](https://www.bournemouth.ac.uk/news/2025-03-12/researchers-warn-addiction-over-dependency-chatgpt)
52. Should Policymakers Regulate Human-AI Relationships? | ITIF, accessed May 7, 2025, [https://itif.org/events/2025/06/11/should-policymakers-regulate-human-ai-relationships/](https://itif.org/events/2025/06/11/should-policymakers-regulate-human-ai-relationships/)
53. Report on the Lack of Safeguards Regarding Potential Cognitive Confusion Among Vulnerable Populations Due to Interactions with ChatGPT - OpenAI Developer Forum, accessed May 7, 2025, [https://community.openai.com/t/report-on-the-lack-of-safeguards-regarding-potential-cognitive-confusion-among-vulnerable-populations-due-to-interactions-with-chatgpt/1238154](https://community.openai.com/t/report-on-the-lack-of-safeguards-regarding-potential-cognitive-confusion-among-vulnerable-populations-due-to-interactions-with-chatgpt/1238154)
54. Is Generative AI Increasing the Risk for Technology‐Mediated Trauma Among Vulnerable Populations? - PMC, accessed May 7, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11773440/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11773440/)
55. How Artificial Intelligence Is Reshaping Relationships | Psychology Today, accessed May 7, 2025, [https://www.psychologytoday.com/us/blog/the-digital-self/202406/how-artificial-intelligence-is-reshaping-relationships](https://www.psychologytoday.com/us/blog/the-digital-self/202406/how-artificial-intelligence-is-reshaping-relationships)
56. www.sacap.edu.za, accessed May 7, 2025, [https://www.sacap.edu.za/blog/applied-psychology/perspectives-on-ai-relationships/#:~:text=Psychological%20Impacts%20of%20AI%20Relationships&text=AI%20companions%20have%20the%20potential,a%20false%20sense%20of%20intimacy.](https://www.sacap.edu.za/blog/applied-psychology/perspectives-on-ai-relationships/#:~:text=Psychological%20Impacts%20of%20AI%20Relationships&text=AI%20companions%20have%20the%20potential,a%20false%20sense%20of%20intimacy.)
57. Full article: Cross-cultural perspectives on AI adoption in teacher education: a comparative study of pre-service teachers in Turkey and the United Arab Emirates - Taylor & Francis Online, accessed May 7, 2025, [https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2488143?src=exp-la](https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2488143?src=exp-la)
58. How reactions to AI are shaped by cultural differences, accessed May 7, 2025, [https://dobetter.esade.edu/en/AI-cultural-differences](https://dobetter.esade.edu/en/AI-cultural-differences)
59. Cross Cultural Examination of Students Attitudes and Intentions Towards AI in Higher Education - INOVASI ANALISIS DATA, accessed May 7, 2025, [https://analysisdata.co.id/index.php/aei/article/view/190](https://analysisdata.co.id/index.php/aei/article/view/190)
60. Cultural Variation in Attitudes Toward Social Chatbots - PubMed, accessed May 7, 2025, [https://pubmed.ncbi.nlm.nih.gov/40160265/](https://pubmed.ncbi.nlm.nih.gov/40160265/)
61. The Formation and Maintenance of AI Intimacy: A Study on Character.AI - University of Warwick, accessed May 7, 2025, [https://warwick.ac.uk/fac/cross_fac/cim/apply-to-study/masters-programmes/digital-media-culture/5541694_-_dmc_dissertation.pdf](https://warwick.ac.uk/fac/cross_fac/cim/apply-to-study/masters-programmes/digital-media-culture/5541694_-_dmc_dissertation.pdf)
62. Responsible AI in Global Health: Solutions from the Global South | Center for Global Digital Health Innovation, accessed May 7, 2025, [https://publichealth.jhu.edu/center-for-global-digital-health-innovation/responsible-ai-in-global-health-solutions-from-the-global-south](https://publichealth.jhu.edu/center-for-global-digital-health-innovation/responsible-ai-in-global-health-solutions-from-the-global-south)
63. Full article: AI-Generated News Content: The Impact of AI Writer Identity and Perceived AI Human-Likeness - Taylor & Francis Online, accessed May 7, 2025, [https://www.tandfonline.com/doi/full/10.1080/10447318.2025.2477739?src=exp-la](https://www.tandfonline.com/doi/full/10.1080/10447318.2025.2477739?src=exp-la)
64. AI-determined similarity increases likability and trustworthiness of human voices | PLOS One, accessed May 7, 2025, [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0318890](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0318890)
65. Ethical AI for Teaching and Learning - Center for Teaching Innovation - Cornell University, accessed May 7, 2025, [https://teaching.cornell.edu/generative-artificial-intelligence/ethical-ai-teaching-and-learning](https://teaching.cornell.edu/generative-artificial-intelligence/ethical-ai-teaching-and-learning)
66. 1. Artificial intelligence in daily life: Views and experiences - Pew Research Center, accessed May 7, 2025, [https://www.pewresearch.org/internet/2025/04/03/artificial-intelligence-in-daily-life-views-and-experiences/](https://www.pewresearch.org/internet/2025/04/03/artificial-intelligence-in-daily-life-views-and-experiences/)
67. Exploring the Ethical Challenges of Conversational AI in Mental Health Care: Scoping Review - XSL•FO, accessed May 7, 2025, [https://mental.jmir.org/2025/1/e60432/PDF](https://mental.jmir.org/2025/1/e60432/PDF)
68. The Hall of AI Fears and Hopes: Comparing the Views of AI Influencers and those of Members of the U.S. Public Through an Interactive Platform - arXiv, accessed May 7, 2025, [https://arxiv.org/html/2504.06016v1](https://arxiv.org/html/2504.06016v1)
69. The Hall of AI Fears and Hopes: Comparing the Views of AI Influencers and those of Members of the U.S. Public Through an Interactive Platform - ResearchGate, accessed May 7, 2025, [https://www.researchgate.net/publication/390601614_The_Hall_of_AI_Fears_and_Hopes_Comparing_the_Views_of_AI_Influencers_and_those_of_Members_of_the_US_Public_Through_an_Interactive_Platform](https://www.researchgate.net/publication/390601614_The_Hall_of_AI_Fears_and_Hopes_Comparing_the_Views_of_AI_Influencers_and_those_of_Members_of_the_US_Public_Through_an_Interactive_Platform)
70. AI chatbots and companions – risks to children and young people | eSafety Commissioner, accessed May 7, 2025, [https://www.esafety.gov.au/newsroom/blogs/ai-chatbots-and-companions-risks-to-children-and-young-people](https://www.esafety.gov.au/newsroom/blogs/ai-chatbots-and-companions-risks-to-children-and-young-people)
71. The ethics of artificial intelligence: Issues and initiatives - European Parliament, accessed May 7, 2025, [https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf](https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf)
72. Self-Regulation in Emerging and Innovative Industries | Published in Houston Law Review, accessed May 7, 2025, [https://houstonlawreview.org/article/129432-self-regulation-in-emerging-and-innovative-industries](https://houstonlawreview.org/article/129432-self-regulation-in-emerging-and-innovative-industries)
73. Media Self-Regulation in the Use of AI: Limitation of Multimodal Generative Content and Ethical Commitments to Transparency and Verification - MDPI, accessed May 7, 2025, [https://www.mdpi.com/2673-5172/6/1/29](https://www.mdpi.com/2673-5172/6/1/29)
74. What Happens When a Companion Chatbot Crosses the Line? - Drexel University, accessed May 7, 2025, [https://drexel.edu/news/archive/2025/May/companion-chatbot-harassment](https://drexel.edu/news/archive/2025/May/companion-chatbot-harassment)
75. Difference in and Influences on Public Opinion About Artificial Intelligence in 20 Economies, accessed May 7, 2025, [https://ijoc.org/index.php/ijoc/article/download/22909/4915](https://ijoc.org/index.php/ijoc/article/download/22909/4915)
76. Survey Highlights an Emerging Divide Over Artificial Intelligence in the U.S., accessed May 7, 2025, [https://comminfo.rutgers.edu/news/survey-highlights-emerging-divide-over-artificial-intelligence-us](https://comminfo.rutgers.edu/news/survey-highlights-emerging-divide-over-artificial-intelligence-us)
77. A conceptual ethical framework to preserve natural ... - Frontiers, accessed May 7, 2025, [https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1377938/full](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1377938/full)
78. standards.ieee.org, accessed May 7, 2025, [https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_general_principles.pdf](https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_general_principles.pdf)
79. Human-AI relationships pose ethical issues, psychologists say ..., accessed May 7, 2025, [https://www.eurekalert.org/news-releases/1079301](https://www.eurekalert.org/news-releases/1079301)