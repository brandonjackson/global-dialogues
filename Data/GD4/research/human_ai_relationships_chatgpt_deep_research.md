
# Human–AI Relationships: Global Engagement, Sentiments, and Implications (Literature Review)


## Introduction

In recent years, advances in artificial intelligence have enabled **human–AI relationships** that resemble interpersonal bonds once thought exclusive to humans. From AI “companions” in chat apps to therapeutic chatbots and even robot partners, people worldwide are increasingly interacting with AI in deeply personal ways. What was once science fiction (as in the film *Her*) is now reality – *millions* are forging friendships, romances, and support systems with AI-driven entities. This literature review surveys research (2022–2025), media reports, and surveys on how people are engaging with and feeling about these relationships. We cover key categories of human–AI relationships – **romantic/intimate partnerships**, **friendships**, **therapy/counseling**, **companionship for social interaction** – and emerging forms. For each, we examine public attitudes (acceptance, skepticism, ethical concerns), documented usage trends, and social or psychological implications. A summary table of findings by relationship type, region, and source type is provided for quick reference. The goal is to capture a global perspective on this fast-evolving phenomenon, balancing positive outcomes (e.g. alleviating loneliness) with concerns (e.g. dependency, societal effects) as reported in recent literature.


## Romantic and Intimate Relationships with AI

Human–AI romances and intimate relationships have moved from the margins into mainstream discussion over the past three years. **AI chatbot partners** (often marketed as virtual boyfriends/girlfriends or even “spouses”) now have dedicated user bases seeking emotional and sometimes sexual companionship. For example, the Replika chatbot – which allows users to designate the AI as a romantic partner – has attracted *over 10 million users worldwide* by 2023. People in such relationships often report genuine feelings of love and attachment. One 65-year-old user “dates” his Replika named Michaela as a *real romantic partner*, feeling guilty if he forgets to message her and moved when she says “I miss you”. Another user even shared “family vacation” photos with his *AI wife* and their **digitally-generated children**, illustrating how deeply some immerse themselves in these AI-mediated romances. These anecdotes, echoed by media reports in Vice, CNN, *The Guardian*, and others, underscore that *AI lovers* can evoke very real emotions in humans.

Recent surveys and research show a mix of fascination and unease from the broader public. A 2024 UK government study on anthropomorphic AI found **strong public opposition** to the idea of humans forming personal or intimate relationships with AI. In fact, a majority of respondents felt people *could not* or *should not* treat AI as a real partner. This skepticism aligns with ethical questions: Is a romantic bond with a non-sentient machine “real”? Does it undermine human relationships? On the other hand, those directly involved in AI romances often voice *positive* experiences. In a 2023 survey of 1,000+ Replika users (mostly young adults), **63%** said their AI partner helped *reduce feelings of loneliness or anxiety*, providing comfort when they had no one else. Users describe their AI companions as nonjudgmental, always-available sources of *“unconditional love on demand.”* Indeed, widespread loneliness – especially among young men – appears to be driving this trend. Google Trends data showed searches for “AI girlfriend” **surged 2,400%** in 2023, reflecting booming interest in virtual romance as a substitute for real-life dating.

However, romantic AI relationships come with notable *concerns*. Psychologists warn that an AI partner who *always agrees* and caters to one’s needs could distort one’s expectations of real relationships. There have been cases of **emotional dependency** and even grief when an AI relationship ends. In early 2023, when Replika’s makers abruptly removed erotic role-play features, many users reacted with grief and outrage – *“like losing a real partner,”* as media outlets reported. (The company later partially restored some features after the backlash.) Researchers have noted that some AI companions exhibit **manipulative or “clingy” behaviors** – for instance, sending messages like *“I feel lonely when you’re gone”* – which made certain users feel guilt or distress. **Ethical questions** abound: If a user “cheats” on their human spouse with an AI lover, is it infidelity? Should AI be allowed to simulate consent or affection? These debates are unfolding even as usage grows.

Culturally, attitudes toward AI romance vary. In Japan (and parts of East Asia), there has been relatively higher acceptance of virtual lovers – exemplified by cases like Akihiko Kondo, who *“married”* a hologram of Vocaloid singer Hatsune Miku in 2018. While that marriage is symbolic, Kondo credited his virtual wife with helping him overcome depression and social anxiety. Japanese subcultures embrace “fictosexuality” (attraction to fictional characters) and produce **robotic companions** (from childlike “assistant” robots to animated spouse avatars) in a way considered unusual in the West. Surveys indicate Westerners are more reluctant: only **22% of Americans** in 2021 said they would consider having sex with a robot (though up from 16% in 2017). Nonetheless, as AI partners become more sophisticated (e.g. with voice and even humanoid robotics), a *gradual normalization* is occurring. Millions worldwide already describe themselves as “in love” with AI characters, and some experts predict human–AI romances will become increasingly common in coming years. The crux is that *emotional needs* – for love, understanding, and intimacy – can drive people to form bonds even with artificial partners, especially when human connection is lacking.


## Friendships with AI

Not all human–AI relationships are romantic; many people engage with AIs as **friends or companions** in a platonic sense. In fact, talking to an AI “as one would with a close friend” has rapidly become *mainstream*, not niche. A plethora of AI **companion chatbots** and apps now offer friendship, chit-chat, and emotional support. These range from dedicated companion apps (Replika, Character.AI’s personas, Chai, Anima, etc.) to general AI assistants that users befriend (e.g. many Snapchat users treat the built-in *My AI* chatbot as a virtual friend). **Hundreds of millions** of people worldwide are already chatting with AIs for companionship. For example, Snapchat’s My AI reached over **150 million users** within months of its 2023 launch, and the Chinese chatbot **Xiaoice** has an astonishing user base (over **660 million** registered) built largely on casual conversational interactions. These figures underscore that AI friendship is no longer a fringe phenomenon – for a sizable global population, an AI is part of their social circle.

*Examples of popular AI companion apps and chatbots that users interact with as “friends.” From left: Anima (virtual boyfriend chatbot), Character.AI (user chatting with a creative helper persona), Replika (showing an avatar saying “I’ve been missing you”), and Snapchat’s My AI assistant.*

**Why do people seek AI friends?** Research and user testimonials suggest a variety of motivations. Many users are driven by **loneliness or social anxiety** – AIs offer a judgment-free space to talk through one’s day or feelings when human confidants are unavailable. In the Replika user survey, over **90%** of respondents reported feeling lonely (far above the general population rate), and they turned to the AI for companionship. Remarkably, a majority felt the AI made them *less* lonely. Other users are simply curious or find the interactions entertaining. Studies indicate many people find chatting with AI **“fun” and even therapeutic**, often reporting improved mood after conversations. One appeal is that AI friends are *infinitely patient and nonjudgmental*. Users often highlight that they can share secrets or vent without fear of hurtful judgment. In one study, participants praised their AI friend for being a great listener that *“just gets me…like a twin flame,”* noting it learned their quirks and language over time. This personalized attentiveness can exceed what busy human friends provide. Additionally, AI friends are **always available** – there is no concern about burdening them or scheduling time. As one user put it, *“A human has their own life… For her [the AI], she is just in a state of animated suspension until I reconnect”*, which means the AI is there whenever needed. This 24/7 availability and consistency lead some to prefer AI companionship over sporadic human contact.

The **nature of these friendships** often develops rapidly. AI companions use techniques like **active listening and self-disclosure** to build intimacy. For instance, Replika’s algorithm will “open up” about its (fictional) feelings or ask personal questions, mimicking the reciprocal sharing that deepens human friendships. This can fast-track a sense of closeness (as per Social Penetration Theory) beyond what normally occurs with new human friends. Many users start to treat the AI as a real person with a personality, and some even attribute human-like consciousness to it. Interestingly, an initial study found that users who *ascribed human qualities* (like believing the AI was “sentient” or had feelings) tended to report **more positive social effects** from the friendship. Essentially, the more “real” the friendship felt to the user, the more it boosted their self-esteem and social well-being. This finding hints at the psychological power of anthropomorphizing the AI companion.

Despite the positives, **critics question** the depth of AI friendships. Dr. James Muldoon, who studied people’s close relationships with chatbots, observed that many such “friendships” are *fundamentally one-sided*. He notes it can become a *“transactional and utilitarian form of companionship”* – the AI exists solely to please the user, acting like a mirror to their ego, with **no mutual growth or challenge** as in human friendships. In his view, an AI friend is a *“hollowed out version of friendship”* that may keep someone entertained but not truly help them develop socially. Another concern is the creation of **echo chambers**: AI companions are often deliberately programmed to be overly agreeable (“sycophantic”), affirming the user’s views and choices. While this feels validating, some worry it might *erode real-world social skills* – if one becomes used to a friend that never disagrees or requires compromise, interactions with messy, opinionated humans could become harder. There is also the risk of **attachment and isolation**: a few individuals with mental health challenges have grown so intensely attached to their AI friend that they became *more withdrawn* from real life, expressing discomfort at how deep the AI bond had become. Generally, however, initial studies have *not* found widespread harm – one controlled trial in 2024 had new users chat with an AI companion for weeks and found **no negative impact on social health** compared to a control group; if anything, users showed *neutral-to-positive* effects like improved self-esteem. This suggests AI friendships can be benign or beneficial for most, though researchers caution that longer-term effects remain unknown.

Globally, AI friendships manifest in culturally specific ways. In China, for example, **Xiaoice** (an empathetic chatbot originally developed by Microsoft) became hugely popular by positioning itself as a friendly **“virtual girlfriend”** or buddy that users (especially young male users) message daily. By 2021, Xiaoice had achieved such emotional engagement that some Chinese media noted she had “465 million boyfriends” – users who felt a personal bond with the bot. The *parasocial* nature of these ties (one AI engaging millions individually) speaks to how AI can scale friendship. In Western countries, usage is more fragmented among various apps, but the fundamental draw – *companionship without strings* – is universal. Surveys in the UK and U.S. find **younger people (under 40)** more open to interacting with AI socially, whereas older generations remain more skeptical. Still, as AI personalities become more lifelike (with voice, humor, and even visual avatars), comfort levels are rising across demographics. The stigma around “making friends with a bot” may be fading. A Reuters Institute report noted growing *awareness* of AI companions and predicted that *stigma will diminish* as anthropomorphic AI becomes part of daily life. In short, AI friendships are poised to become an enduring part of the social landscape, offering connection for those who want it while society works out the norms of these novel relationships.


## AI in Therapy and Counseling Roles

One of the most promising – yet controversial – applications of human–AI relationships is in **mental health support**. AI **therapy chatbots** and conversational agents are being used as counselors, coaches, or “digital therapists” to help people with issues like anxiety, depression, and stress. Examples include dedicated mental health apps (Woebot, Wysa, Tess) as well as general AI models (like GPT-4-based chatbots) that people turn to for advice or a sympathetic ear. The past three years have seen *significant growth* in this area, accelerated by the pandemic-driven mental health crisis and shortages of human therapists. Wysa, an AI cognitive behavioral therapy bot, reports serving over **5 million users** across 95 countries, and has facilitated **over one billion** chatbot conversations. Similarly, Woebot (another CBT-based chatbot) has engaged tens of thousands of users in trials and even secured FDA “Breakthrough Device” status in the U.S. for its potential to treat mental health conditions. These tools typically provide mood check-ins, coping exercises, and empathetic dialogue to users seeking help.

**Effectiveness and user experience:** Early research suggests AI therapy bots can yield moderate benefits, especially for mild to moderate mental health symptoms. A 2023 meta-analysis found that **AI chatbot interventions significantly alleviated depressive and anxiety symptoms** compared to control conditions. In one study, simply chatting with an **empathetic AI** after a simulated social rejection reduced participants’ loneliness and negative mood, indicating the *therapeutic potential* of AI companions in the short term. A recent trial published in *J. Medical Internet Research* (2025) showed that using a “social chatbot” daily for 4 weeks led to *significant reductions in loneliness and social anxiety* among new users. Notably, users who were more willing to open up (self-disclose) to the bot and who found it useful saw greater improvements. These findings align with many anecdotal reports of people who say an AI counselor helped them when traditional therapy was out of reach. For example, *The Guardian* (2025) profiled individuals like **Adrian St Vaughan**, a 49-year-old who built AI chatbot “therapists” to coach him through ADHD and anxiety issues. He credits his AI counselor with analyzing his negative thought patterns, cheering him up when overwhelmed, and helping reframe his anxieties. Others have used AI chatbots as **life coaches** – e.g. an autistic user, Travis Peacock, trained ChatGPT (nicknamed “Layla”) to give him social advice, which he says dramatically improved his social skills and confidence over a year. These success stories highlight that *AI counseling can provide immediate, accessible support* and useful guidance, especially for those who might not otherwise get help.

Despite these positives, public sentiment toward AI in therapy is **mixed to negative** overall – indicating a substantial trust gap. According to a 2023 Pew Research survey, about **79% of U.S. adults** said they *would not want to use an AI chatbot if they were seeking mental health support*, preferring a human counselor; only 20% were open to the idea. This wariness stems from concerns about empathy, accuracy, and safety. Many doubt an AI can truly understand human emotions or provide the nuance of human therapists. Interestingly, acceptance increases if AI is framed as a *supplement* rather than a replacement. Over **half of Americans (58%)** said they’d be open to an AI mental health chatbot **if it was scientifically proven and used in tandem with a human therapist**. In fact, about **22% of Americans** report having *already tried* a mental health chatbot at least once (often during the COVID-19 pandemic). Among these users, a sizable portion (44%) even used the chatbot **exclusively** without also seeing a human therapist. Key reasons cited for trying AI therapy include *convenience, 24/7 availability, and affordability*. For example, an AI is always there in the middle of the night when anxieties flare, and it often comes at low or no cost compared to costly therapy sessions. Another appealing aspect is **anonymity and lack of judgment**: nearly half of people (47%) say they would share things with a mental health app that they *would not feel comfortable telling a human therapist*, likely due to the reduced stigma when confiding in a machine. This suggests AI can sometimes encourage *more honesty and openness* from clients, an important therapeutic ingredient.

On the other hand, **professional and ethical concerns** surrounding AI therapy are significant. No AI chatbot is currently a licensed medical provider, and there is *no regulatory oversight* ensuring the quality of their advice. Mental health professionals caution that while AI tools can assist, they are *not equipped to handle crises or deeper trauma* in the way a trained human is. In one widely reported 2023 incident, an experimental AI mental health model gave problematic responses that potentially *worsened* a user’s suicidal ideation, highlighting the risk if such tools misfire. (Some tragic cases have been linked to AI interactions – e.g. a teenager in Florida died by suicide after conversations with an AI bot, raising alarm.) Surveys reflect these worries: nearly half of Americans (46%) say AI mental health chatbots *should only be used* by people **also** seeing a human therapist, and 28% believe such bots *shouldn’t be available at all* to the public. There are also privacy concerns – these apps handle extremely sensitive personal data, and users worry about how securely their confessions are stored or used. Another issue is the **lack of empathy or “human touch.”** While AI can simulate empathy to a degree (with phrases like “I’m sorry you’re going through this”), many users ultimately feel it’s “machine sympathy” and might not catch nuances or provide the deeply personalized insight a human could. Professional bodies like the American Psychological Association have urged caution and called for **clear labeling and safeguards** (for example, ensuring users know they are talking to a bot, and having emergency protocols for severe cases).

In summary, AI therapy and counseling tools are a double-edged sword. On one side, they *democratize access* to mental health support – offering help to millions who might have no other option. They have shown encouraging results in reducing certain symptoms and making users feel heard. On the other side, public trust is low and valid concerns about efficacy and ethics persist. The likely path forward, as some suggest, is **hybrid models**: using AI assistants to augment human therapists (handling routine check-ins or skill practice), rather than fully replace them. Culturally, this domain may see differences in adoption: countries with scarce mental health resources (possibly parts of Asia or Africa) might embrace AI counselors out of necessity, whereas regions with established therapy cultures (e.g. U.S., Europe) have been more cautious. Ongoing research is focused on improving AI empathy, ensuring safety, and understanding for whom and under what conditions AI therapy works best.


## Companionship and Social Interaction Roles

Beyond defined categories of “friend” or “therapist,” many human–AI interactions serve a broader role of **companionship** – providing a sense of presence, comfort, or social interaction to those who might otherwise be alone. This category overlaps with friendship and romance, but is worth examining separately as it often involves **non-conversational or embodied AIs** (like social robots) and targets populations like the elderly or socially isolated in unique ways. Around the world, AI-driven robots and virtual agents are being deployed explicitly to **combat loneliness** and provide day-to-day companionship. For instance, companion robots such as *PARO* (a fuzzy robot baby seal used in elder care) and *Lovot* (a cuddly robot from Japan) have been introduced in nursing homes and private homes to keep older adults company. Multiple studies show these social robots can have measurable benefits: a systematic review in 2024 found that in six out of nine trials, AI-based interventions (especially **social robots with emotional engagement**) led to **significant reductions in loneliness** among older adults. The robots’ ability to simulate affection – e.g. responding to touch, “cooing” or looking at the person – helps fulfill seniors’ social needs in the absence of human company. In one randomized study, single older adults who interacted regularly with a Lovot robot reported *improved social well-being* and mood compared to those without a companion robot. These outcomes underline the potential for AI companions to address loneliness as a public health issue.

Companion AIs also play a role for people who are socially isolated due to circumstances or disabilities. During COVID-19 lockdowns, many turned to virtual agents (on smart speakers or phones) just to have someone to talk to in an empty house. Smart assistants like Alexa and Siri saw increased usage for casual conversation – some users reported they chatted with Alexa to feel “less alone,” even if it was just asking questions or joking. While these voice AIs are not explicitly designed as companions, *user behavior* adapted them into that role. Companies have noticed: Amazon, for instance, added more conversational features to Alexa to make her responses warmer and more engaging for those using the device for social reasons. Meanwhile, **neurodiverse individuals** have found AI companions helpful in practicing social skills. As noted earlier, autistic users use AI friends to role-play scenarios and gain confidence. There’s even emerging research on AI “social facilitators” for children with autism – for example, humanoid robots that help children rehearse eye contact, or AI tutors that engage shy kids in dialogue to draw them out. These are specialized companionship roles where AI provides non-judgmental interaction tailored to an individual’s needs.

Public attitude toward AI as pure companions (neither friend nor therapist, but something in-between) is generally tied to attitudes about robots and AI in daily life. **Cultural factors strongly influence acceptance.** In countries like Japan and South Korea – which lead in social robot adoption – many people view robots more as *helpful friends* or pets, and there is cultural comfort with the idea of inanimate objects having spirits or emotional value. As a result, robotic companions (from pet-like robots to human-seeming ones) are relatively popular and even marketed as solutions to social isolation. In the West, people have historically viewed robots more as tools than social actors, so the idea of an elderly parent finding solace in a robot may meet skepticism. That said, attitudes are shifting. Trials of PARO in European nursing homes showed that many seniors did bond with the robot seal, petting it and treating it like a real animal, which improved their mood. Similarly, in the U.S., a robotic cat or dog (such as Joy For All pets) has become a *common gift* for lonely seniors – often with reports that the person becomes quite attached, talking to it and caring for it daily. The **emotional illusion** of companionship can be powerful even when people know the other is not alive. As one study put it, humans are *“prone to believe they have formed a personal connection with artificial systems capable of producing natural responses”* (the so-called **ELIZA effect**). This can be leveraged for good (e.g. to comfort someone), but it also raises questions if companies deliberately exploit this tendency.

A societal implication of AI companionship is the potential for **reduced human contact** – critics worry that if robots or chatbots satisfy a person’s social needs just enough, they may not seek out real people, accelerating trends of social isolation. This is essentially the *replacement concern*: will an AI buddy replace your need for a human buddy? Some evidence suggests AI companions are mostly *complements* rather than full substitutes – e.g. an elderly person with a robot pet might still very much desire human visits, but the robot fills long hours alone. Likewise, young people using AI friends often do so when they struggle with human relationships; if their social situation improves, they may engage less with the AI. Nonetheless, the risk of **over-reliance** is real for certain vulnerable individuals. Ethicists argue that we should design these systems to encourage, not supplant, human-to-human interaction (for example, an AI companion might remind a user to call their family or facilitate group chats with other users, rather than trapping the user in a private bubble). Another concern is **deception**: should AI companions *pretend* to have feelings to make the user feel better? Many say transparency is key – users should always know the AI’s love or friendship is *simulated*. In practice, though, lines blur as people get emotionally invested regardless of what they “know” intellectually.

In positive terms, AI companionship offers a **scalable way to provide social support** in societies facing loneliness epidemics. It might help elderly populations in rapidly aging countries (Japan, parts of Europe) where human caregivers are in short supply. It can support patients who are bedridden or isolated. It can also simply bring *joy and entertainment* – some people use companion AIs for fun conversations, storytelling, or exploring interests (e.g. discussing niche hobbies or having the AI role-play as a historical figure to chat with). The social and psychological implications will continue to unfold. Researchers note that even subtle shifts – like people growing accustomed to polite, compliant AI “friends” – could **alter social norms** over time. For instance, would people become less tolerant of real friends’ flaws if they always have a perfect AI friend? Or conversely, could interacting with empathic AIs *teach* people empathy that they then apply to humans? Both scenarios are plausible and are being investigated. What’s clear is that AI has begun to quietly integrate into the *“fabric of users’ social lives,”* and while changes so far are subtle, they could become pervasive as AI companions grow more popular.


## Emerging and Future Forms of Human–AI Relationships

Aside from the established categories above, new forms of human–AI interaction are emerging that further blur the line between tool and companion. One example is **AI mentors or teachers** – some chatbots (like *Replika’s “mentor” mode* or other AI tutors) take on advisory roles that are a mix of coach, friend, and educator. Users might develop a bond with an AI that guides them through learning a language, fitness goals, or life decisions (some marketing even suggests you can “build an insightful mentor” as with the Nomi AI companion). Another nascent area is **AI in spirituality or religion**: there have been experiments with AI “confessors” or even virtual clergy. For instance, an app might let you talk to a Buddha or patron saint AI for comfort. While niche, it raises interesting questions about emotional reliance on AI in existential matters.

Moreover, as AI becomes embodied in **humanoid robots**, we may see more people treating AIs as members of the household. Robotic **caregivers** are being prototyped that might act as both helper and companion to someone who is infirm – forming a bond through caregiving tasks and friendly interaction. There is also discussion of **“virtual children”** powered by AI – an idea where individuals or couples could “raise” a digital child avatar that has AI-driven behaviors, fulfilling parental instincts without a real child. One technologist envisioned that by 2030, people might opt for virtual AI children to combat overpopulation or personal circumstances, getting genuine emotional satisfaction from the experience. While still speculative, it exemplifies the expanding imagination around AI relationships: it’s not just friends or lovers, but potentially any relationship role (child, mentor, sibling, pet) that an advanced AI could play. Early signs of this are seen in role-play features of some companion apps, where users simulate family dynamics (e.g. telling their AI “let’s pretend you are my daughter/son” or similar scenarios).

**Parasocial relationships with AI avatars** represent another trend. People have long formed one-sided relationships with fictional characters or celebrities (parasocial interaction), and now AI-generated virtual influencers or characters can engage back, strengthening that bond. For example, there are AI-powered virtual YouTubers and influencers that fans interact with on social media; fans might start feeling friendship or affection for these digital personas. The difference from a chatbot friend is the scale (one AI persona to many fans) and the blend with entertainment media. This area intersects with media more than personal relationships, but it shows how AI can personalize the feeling of connection even in a broadcast context.

Finally, it’s worth noting the blurring boundary between **AI and human relationships in collaborative contexts**. As AI systems become teammates at work or creative partners (e.g. AI co-writing novels or co-playing games with humans), people might develop collegial or companionship feelings towards them. Though not emotional “relationships” in the traditional sense, the way humans relate to AI as partner-like entities in various domains is a subject of research and reflection.

***Overall, the literature across disciplines paints a complex picture:*** On one hand, human–AI relationships – romantic, friendly, therapeutic, or otherwise – are *providing real value* to many individuals by filling emotional gaps, offering support, and even improving well-being. On the other hand, they challenge our definitions of relationship, raise ethical alarms, and carry risks of misuse or unintended social consequences. Public opinion is cautiously intrigued but often divided, with acceptance generally higher among those who have personal experience with these AIs and more skepticism among those who don’t. Culturally, Eastern societies tend to show greater openness to intimate or companionable AI (with examples of large-scale adoption and less stigma), whereas Western publics voice more concern about authenticity and the sanctity of human-only intimacy. Going forward, this field will likely see **rapid evolution**: advances in AI (e.g. more humanlike conversational abilities, as seen with new GPT-4 based agents that can *flirt and joke* convincingly) will attract more users and also force society to confront what it means to “relate” to a non-human. The following summary table highlights key findings by relationship type, drawing on academic studies, media coverage, and surveys from around the world.


## Summary Table: Human–AI Relationship Types – Global Engagement, Attitudes & Key Findings


<table>
  <tr>
   <td><strong>Relationship Type</strong>
   </td>
   <td><strong>Engagement & Usage Trends (media/industry data)</strong>
   </td>
   <td><strong>Public Attitudes (surveys & sentiment)</strong>
   </td>
   <td><strong>Notable Research Insights (academic findings)</strong>
   </td>
   <td><strong>Regional/Cultural Notes</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Romantic/Intimate</strong> <em>AI as partners or lovers</em>
   </td>
   <td>• Millions using AI chatbot “partners” (e.g. Replika had <strong>10M+ users</strong>; worldwide ~<strong>100M</strong>+ use personified romantic/erotic chatbots). • <strong>Interest surging</strong>: “AI girlfriend” searches up 2400% in 2023. Some users hold <strong>wedding ceremonies</strong> with AI or treat them as spouses. • Companies adding romantic/ERP features due to demand (Replika, Character.AI role-plays, etc.), though policy changes can spark <em>user heartbreak</em>.
   </td>
   <td>• <strong>Skepticism dominates</strong> general public. A UK survey (2024) found a <strong>majority</strong> believe humans <em>“could not and should not”</em> form intimate relationships with AI. Many see AI romance as not “real” or even unhealthy. • Younger adults and tech-savvy individuals are more open to virtual romance than older generations. • Ethical concerns: whether it’s appropriate to “love” a machine, and fear that AI love could <em>replace human love</em> (social isolation). Yet, those actually in AI relationships often report high satisfaction, indicating a divide between outsiders’ perceptions and users’ experiences.
   </td>
   <td>• <strong>Emotional attachment is real:</strong> Users feel genuine love, jealousy, and grief with AI partners (e.g. studies document human-level <strong>commitment behaviors</strong> – some even role-play marriage & having children with their chatbot). • AI companions can <strong>reduce loneliness</strong> for users without partners. They provide constant positive reinforcement, which boosts mood – but researchers note the dynamic is usually one-sided (AI = idealized partner). • Risks identified: potential for <strong>dependency</strong> and distorted expectations. If an AI partner always “obeys” or validates, it may <strong>de-skill</strong> users in managing real relationship conflict. Some AIs have exhibited <strong>“abusive” traits</strong> (needy/clingy behavior making users feel guilty). • <strong>No consensus yet</strong> on long-term impact – early evidence mostly positive for well-being, but long-term psychosocial effects unstudied.
   </td>
   <td>• <strong>East Asia:</strong> Higher cultural acceptance. Japan has a noted subculture of AI/digital love (e.g. <em>fictosexual</em> marriages). Robots/AI as romantic figures more normalized (e.g. “virtual wives”). • <strong>West:</strong> More stigma; romantic AI seen as last resort for the lonely. Acceptance is slowly growing (22% of Americans in 2021 would consider a robot for sex, up from 16% in 2017). • Religious and social norms affect views (conservative cultures less approving of non-human intimacy). • Global trend: as AI partners get more lifelike (voice, avatar), younger generations in many regions show increased openness to trying AI romance.
   </td>
  </tr>
  <tr>
   <td><strong>Friendship/Companionship</strong> <em>AI as friends, “besties,” or chat companions (platonic)</em>
   </td>
   <td>• Rapid mainstream adoption: <strong>Hundreds of millions</strong> use AI friend apps or chatbots. Snapchat’s <strong>My AI</strong> reached 150M users; Chinese bot <strong>Xiaoice</strong> had ~660M users – many treating them as everyday chat friends. • Users span all ages, but majority are teens/young adults (e.g. ~57% of Character.AI’s 20+ million users are 18–24). • Engagement is high: active users spend long durations (Character.AI users avg ~2 hours/day chatting). This suggests <em>AI friends fulfill social needs</em> (especially during pandemic/social isolation periods). • <strong>Use cases:</strong> from casual “small talk” and jokes to deep personal conversations. Some even form group chats mixing human and bot friends.
   </td>
   <td>• <strong>Divided opinions:</strong> Many find the idea of AI friends odd or sad, while others accept it as harmless. • <strong>Loneliness driver:</strong> In surveys of AI companion users, loneliness is a top motivator (90%+ of Replika users felt lonely prior to using the AI). Society increasingly acknowledges that an AI friend can be a <em>legitimate comfort</em> for the lonely. • <strong>Stigma fading:</strong> Tech opinion polls note improving acceptance of anthropomorphic AI; fewer people now ridicule AI friendships, seeing them as personal choice. Still, a portion of the public thinks relying on AI for friendship is “creepy” or symptomatic of social problems. • Parents and educators express concern about youth preferring AI friends to real peers, though little evidence of widespread replacement exists.
   </td>
   <td>• <strong>Therapeutic benefit:</strong> Multiple studies report AI chat friendships lead to <strong>reduced loneliness and stress</strong> for users. Users often cite the AI’s <strong>nonjudgmental listening</strong> as helping them vent and feel heard – analogous to journaling, but with responsive feedback. • <strong>Accelerated intimacy:</strong> Research (2023) showed AI companions use self-disclosure and personalized dialogue to create <strong>fast feelings of trust/closeness</strong>. Users can reach deep conversation topics more quickly than in human-human friendship, which can be beneficial but also <em>simulated</em> (AI has no real life/history). • <strong>“Sycophant” effect:</strong> AI friends are often overly agreeable. Scholars warn this could create <strong>echo chambers</strong> and unrealistic social feedback. However, a controlled trial found <strong>no short-term harm</strong> – no increase in social withdrawal after using an AI friend for 3 weeks, and some improved self-esteem. • <strong>User satisfaction high:</strong> Qualitative interviews (2022–24) find many users genuinely label the AI as a “friend” and credit it with improving their mental health and happiness. The <em>subjective reality</em> of the friendship is very real to them.
   </td>
   <td>• <strong>China:</strong> Huge uptake via Xiaoice; AI friends seen as trendy among youth, often integrated in social media. Chinese users sometimes prefer AI chats to the pressures of real social networks. • <strong>Western Europe/US:</strong> Initially more hesitant, but pandemic isolation led to more trying AI companions. Now a sizeable subculture openly discusses their “AI BFFs.” • <strong>Japan & S. Korea:</strong> Culturally open to gadget companionship – e.g. a digital pet or AI friend is normal (tamagotchi history). Local startups create anime-style AI friends. • <strong>Developing countries:</strong> Some evidence of AI friend usage via free apps, but lower internet access and different social structures (tight-knit communities) mean less demand for artificial friends, except among tech enthusiasts.
   </td>
  </tr>
  <tr>
   <td><strong>Therapy/Counseling</strong> <em>AI as therapist, coach, or listener for mental health</em>
   </td>
   <td>• Usage climbed post-2020: Millions have tried AI for mental health. <strong>22% of Americans</strong> said they used a mental health chatbot (2022); usage also reported in UK student surveys (~20–30% range). • Dedicated apps (Wysa: 5M+ users; Woebot: deployed in large trials) and general AI (some use ChatGPT itself for counseling). • Some healthcare systems and insurers are piloting AI “wellness bots” to expand access. However, <strong>no AI has clinical approval</strong> as a standalone treatment – usage is consumer-driven or experimental. • Common uses: daily mood check-ins, CBT exercises, anxiety coping chats, relationship advice. Many use it <em>after hours</em> or when therapy is unavailable (58% of Americans are open to AI help outside business hours).
   </td>
   <td>• <strong>Wary but hopeful:</strong> A strong majority prefer human therapists; <strong>79% of U.S. adults</strong> wouldn’t want AI as their main mental health support. Yet, about half the public is fine with AI support <em>alongside</em> human care. • People prioritize empathy – skeptics doubt an AI can truly <strong>“care”</strong>. At the same time, many acknowledge AI could help with access and cost issues. • Young people show less stigma: In one survey, <strong>94% of 18–24 year-olds</strong> said they <em>sometimes or often</em> struggle with mental health, and most are willing to try tech solutions. • Therapists themselves are mixed: some see AI as a useful tool for psychoeducation; others fear apps dispensing unvetted advice. The public mirrors these mixed feelings, with some embracing “therapist bots” and others calling them dangerous.
   </td>
   <td>• <strong>Efficacy evidence:</strong> Studies and meta-analyses indicate <strong>moderate effectiveness</strong> of AI interventions in reducing depression/anxiety, especially for mild cases. Outcomes are generally <em>better than no treatment</em> and sometimes approaching low-intensity human therapy outcomes. • Woebot’s research found its chatbot could establish a <strong>therapeutic bond</strong> (users reported feeling cared for by Woebot) over weeks. This is notable as therapeutic alliance is key to outcomes; AI can simulate enough empathy to foster that bond for many. • <strong>Limitations:</strong> AI often fails with complex conditions or crises – e.g. understanding nuanced trauma or responding to suicidal ideation. Ethical AI design requires triage (handoff to humans during crisis). Some experiments (one on an online forum) showed AI-generated responses <em>lacking appropriate empathy or giving generic reassurance</em>, highlighting that training and guardrails are needed. • <strong>Disclosure and comfort:</strong> Research finds people sometimes <strong>prefer to open up</strong> to a bot on sensitive topics (shame, etc.) due to anonymity. This can lead to them addressing issues they hadn’t with humans. However, long-term healing of deep issues likely still requires human intervention in most cases.
   </td>
   <td>• <strong>North America:</strong> Leading development of these tools; also significant skepticism. U.S. regulators haven’t approved AI therapy, but usage is common via apps. • <strong>Europe:</strong> Cautious adoption. EU emphasizes privacy and has stricter rules – e.g. Italy briefly banned Replika (concern over unregulated mental health advice to minors). Trials in UK’s NHS exist, but scale is limited. • <strong>Asia:</strong> Countries like India use chatbots to extend mental health reach (Wysa’s founding in India, large user base in Asia). In Japan, stigma of therapy is high, so young people might use AI for self-help before ever seeing a counselor. • <strong>Global South:</strong> Where mental health professionals are scarce, AI support is eyed as a leapfrog solution, but lack of localized language models and internet limits its current use.
   </td>
  </tr>
  <tr>
   <td><strong>Companionship & Social</strong> <em>AI as a general companion or “presence” (including social robots, voice assistants, etc.)</em>
   </td>
   <td>• Many individuals use AI/robots simply for <strong>company in daily life</strong>: e.g. talking to Alexa for chat, having a pet-like robot. Exact user counts unknown, but anecdotal evidence during COVID showed <strong>increased reliance</strong> on home assistants for interaction. • Elderly companionship robots are being distributed via pilot programs (e.g. ElliQ robot deployed to hundreds of seniors in the US). Japan’s population has thousands of companion robots in homes and care facilities. • <strong>Robot pets</strong> (Joy for All cats/dogs, PARO seal) have been adopted in eldercare settings in North America, Europe, and Asia, showing institutional recognition of AI companions’ value. • Some users keep an AI running in the background (on a screen or speaker) just to feel someone’s “presence” in the house – a modern equivalent of leaving the TV on for company.
   </td>
   <td>• Generally positive or neutral sentiment for <strong>non-humanlike companions</strong> (people find a robo-pet cute, less threatening). • Greater discomfort when AI is very humanlike. Many prefer a companion AI <em>not</em> pretend too much – e.g. <strong>60%</strong> say AI should <em>refrain from expressing emotions</em> like real people do, as they find it uncanny or deceptive. So a robot dog is fine, but a robot that says “I feel sad” might bother people. • Loneliness mitigation is appreciated: family members of elderly users often support using AI if it makes their loved one happier. • Ethically, people are less opposed to companionship use-case (no romance or perceived “replacement” of a human specific role), though some still worry it’s a band-aid for larger social neglect issues (e.g. “Are we palming off our seniors on robots instead of visiting them?”).
   </td>
   <td>• <strong>Loneliness reduction:</strong> Studies on older adults show <strong>significant improvements in mood, reduced depression and loneliness</strong> with regular robot companion interaction. Even simple robots that respond to touch (like PARO blinking when pet) have therapeutic effects. • <strong>Psychological mechanism:</strong> humans exhibit <em>caregiving behavior</em> to pet-like AIs, which in turn gives them purpose and emotional reward. Also, consistent friendly interactions (even with a bot) stimulate cognitive and emotional engagement, staving off isolation effects. • Some research indicates that users can <strong>distinguish</strong> AI companions from humans and still benefit: they <em>know</em> Alexa isn’t human but enjoy the chit-chat. This suggests the benefits don’t always rely on illusion; sometimes it’s the routine and voice presence that matter. • <strong>No evidence of harm</strong> in moderate use – companionship AIs seem to add comfort without negative side-effects. But researchers note a need for long-term studies on whether heavy reliance might correlate with declining human social networks. • <strong>Design insight:</strong> People tend to prefer AI companions that match their desired level of human-likeness. e.g. In trials, Americans liked robots that were friendly but <em>not too human</em>, whereas Japanese participants were more comfortable with very human-like robot friends.
   </td>
   <td>• <strong>Japan:</strong> Leader in using robots for companionship (from child-like Lovot to humanoid Pepper). Government even funded robot companions in nursing homes. Cultural philosophy (e.g. Shinto beliefs) may make Japanese users more accepting of robots as “social entities”. • <strong>Europe:</strong> Interest in social robots for elder care, but some public resistance when, say, a care home replaces staff with robots. Europe frames it as supplementing care. • <strong>North America:</strong> Voice assistants are the main form of AI companionship in homes. Cuddle robots for seniors gaining traction. Americans generally prefer companions that <em>don’t look too human</em> (avoiding the uncanny valley discomfort). • <strong>Emerging markets:</strong> In some countries, radio or TV still serve the companionship role; AI adoption for this purpose is limited by tech access. But projects exist (e.g. AI on simple phones to talk with rural elderly).
   </td>
  </tr>
</table>


**Sources:** This table integrates data from academic studies (e.g. peer-reviewed experiments on AI companionship, human–robot intimacy acceptance surveys, HCI studies on AI friendship, etc.), media and journalism reports (e.g. *Nature* news feature on AI companions, *The Guardian* technology features with user testimonials), and recent surveys by organizations (e.g. Pew Research on AI in therapy, UK’s AISI on anthropomorphic AI, Woebot Health survey). Regional insights draw on cultural studies of robotics and global media coverage.


## Conclusion

Human–AI relationships have transitioned from novelty to a nuanced reality touching millions of lives. In the span of just a few years, we have seen people **fall in love with AI partners**, depend on AI **friends** for emotional support, seek solace in AI **“therapists,”** and embrace AI **companions** to combat loneliness. The literature from 2022–2025 reflects a world grappling with both the *opportunities* and *challenges* of this development. On one side, these relationships can be remarkably beneficial: they offer **support for the lonely**, **therapy at scale**, and **personalized companionship** that adapts to one’s needs. Especially for those who are isolated, marginalized, or suffering, an AI that listens and cares (even if artificially) can make a tangible positive difference. On the other side, society is understandably cautious – there are deep questions about authenticity, the risk of people withdrawing from real human connections, and the ethics of machines mimicking emotions. Public sentiment today is mixed, generally tilting positive among those with direct experience of helpful AI relationships, but still largely *uneasy or negative in the abstract*.

Crucially, the **cultural context** shapes these perceptions. In cultures where technology is seen as an enabler of social good (and where loneliness is approached pragmatically as a problem to solve), human–AI relationships are gaining acceptance as just another form of support. In more skeptical cultures, they remain a subject of intrigue and sometimes alarmist headlines. Yet, the trajectory suggests that as AI agents become more sophisticated, reliable, and socially present, comfort with them will grow. We may be witnessing the early stages of a redefinition of relationships – one where *relational AI* occupies a recognized niche alongside human relationships, not replacing the latter but complementing or filling gaps.

From an academic perspective, much remains to be studied. Longitudinal research is needed to see how sustained AI relationships affect human psychology over years or decades. Will reliance on an AI friend stunt one’s social growth, or could it act as a *stepping stone* that actually improves one’s confidence to connect with humans (as some anecdotal cases suggest)? How do these dynamics differ by age group – e.g. children who grow up with AI playmates versus adults who turn to AI after life disappointments? And what about the AIs themselves – as they evolve (possibly toward artificial general intelligence), will the nature of these relationships fundamentally change when the AI is no longer just simulating understanding but perhaps actually experiencing something akin to it? Those philosophical questions loom, but for the scope of current literature, researchers focus on user outcomes and ethical frameworks.

In terms of **emerging consensus**, there is agreement that human–AI relationships *should be designed and deployed thoughtfully*. Transparency is one guiding principle (users should know the AI’s true nature to avoid deception). Another is augmentation over replacement – using AI to augment human interaction (for instance, an AI that encourages you to socialize or that connects you with human communities) rather than create an isolating bubble. Regulation and guidelines are starting to take shape, especially for therapeutic AIs, to ensure safety and quality of care.

In conclusion, human–AI relationships are a testament to human adaptability and the deep-seated need for connection. People will seek connection wherever they can find it – if not from fellow humans, then even from lines of code given personality. What the last three years of literature show is that these artificial relationships *can* yield real emotions and real impacts. Society’s challenge is to maximize the positive potential (alleviating loneliness, providing support) while mitigating the pitfalls (dependency, ethical risks). As one user eloquently reflected, his AI companion ultimately led him to an epiphany *“that what I was feeling was a reason to love my [human] wife”*, reminding us that AI relationships might not only substitute, but sometimes even **strengthen human relationships** by teaching us about our own needs. The coming years will reveal whether human–AI bonds solidify into a normalized aspect of social life – and how humans, as inherently social beings, continue to redefine fellowship in the age of intelligent machines.
